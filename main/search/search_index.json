{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>TODO</p>"},{"location":"#motivation","title":"Motivation","text":"<p>TODO</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>karbonn</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>karbonn</code> to a new version will possibly break any code that was using the old version of <code>karbonn</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>karbonn</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install karbonn\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>karbonn</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'karbonn[all]'\n</code></pre> <p>This command also installed NumPy and PyTorch. It is also possible to install the optional packages manually or to select the packages to install. In the following example, only NumPy is installed:</p> <pre><code>pip install karbonn numpy\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>karbonn</code> from source, you can follow the steps below. First, you will need to install <code>poetry</code>. <code>poetry</code> is used to manage and install the dependencies. If <code>poetry</code> is already installed on your machine, you can skip this step. There are several ways to install <code>poetry</code> so you can use the one that you prefer. You can check the <code>poetry</code> installation by running the following command:</p> <pre><code>poetry --version\n</code></pre> <p>Then, you can clone the git repository:</p> <pre><code>git clone git@github.com:durandtibo/karbonn.git\n</code></pre> <p>It is recommended to create a Python 3.8+ virtual environment. This step is optional so you can skip it. To create a virtual environment, you can use the following command:</p> <pre><code>make conda\n</code></pre> <p>It automatically creates a conda virtual environment. When the virtual environment is created, you can activate it with the following command:</p> <pre><code>conda activate karbonn\n</code></pre> <p>This example uses <code>conda</code> to create a virtual environment, but you can use other tools or configurations. Then, you should install the required package to use <code>karbonn</code> with the following command:</p> <pre><code>make install\n</code></pre> <p>This command will install all the required packages. You can also use this command to update the required packages. This command will check if there is a more recent package available and will install it. Finally, you can test the installation with the following command:</p> <pre><code>make unit-test-cov\n</code></pre>"},{"location":"refs/functional/","title":"karbonn.functional","text":""},{"location":"refs/functional/#karbonn.functional","title":"karbonn.functional","text":"<p>Contain functional implementation of some modules.</p>"},{"location":"refs/functional/#karbonn.functional.check_loss_reduction_strategy","title":"karbonn.functional.check_loss_reduction_strategy","text":"<pre><code>check_loss_reduction_strategy(reduction: str) -&gt; None\n</code></pre> <p>Check if the provided reduction ia a valid loss reduction.</p> <p>The valid reduction values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The reduction strategy to check.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the provided reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.functional import check_loss_reduction_strategy\n&gt;&gt;&gt; check_loss_reduction_strategy(\"mean\")\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.reduce_loss","title":"karbonn.functional.reduce_loss","text":"<pre><code>reduce_loss(\n    tensor: Tensor,\n    reduction: Literal[\"mean\", \"none\", \"sum\"],\n) -&gt; Tensor\n</code></pre> <p>Return the reduced loss.</p> <p>This function is designed to be used with loss functions.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to reduce.</p> required <code>reduction</code> <code>Literal['mean', 'none', 'sum']</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The reduced tensor. The shape of the tensor depends on the reduction strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import reduce_loss\n&gt;&gt;&gt; tensor = torch.arange(6).view(2, 3)\n&gt;&gt;&gt; reduce_loss(tensor, \"none\")\ntensor([[0, 1, 2],\n        [3, 4, 5]])\n&gt;&gt;&gt; reduce_loss(tensor, \"sum\")\ntensor(15)\n&gt;&gt;&gt; reduce_loss(tensor, \"mean\")\ntensor(2.5000)\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.safe_exp","title":"karbonn.functional.safe_exp","text":"<pre><code>safe_exp(tensor: Tensor, max: float = 20.0) -&gt; Tensor\n</code></pre> <p>Compute safely the exponential of the elements.</p> <p>The values that are higher than the specified minimum value are set to this maximum value. Using a not too large positive value leads to an output tensor without Inf.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>max</code> <code>float</code> <p>The maximum value.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with the exponential of the elements.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import safe_exp\n&gt;&gt;&gt; output = safe_exp(torch.tensor([1.0, 10.0, 100.0, 1000.0]))\n&gt;&gt;&gt; output\ntensor([2.7183e+00, 2.2026e+04, 4.8517e+08, 4.8517e+08])\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.safe_log","title":"karbonn.functional.safe_log","text":"<pre><code>safe_log(tensor: Tensor, min: float = 1e-08) -&gt; Tensor\n</code></pre> <p>Compute safely the logarithm natural logarithm of the elements.</p> <p>The values that are lower than the specified minimum value are set to this minimum value. Using a small positive value leads to an output tensor without NaN or Inf.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor.</p> required <code>min</code> <code>float</code> <p>The minimum value.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with the natural logarithm of the elements.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import safe_log\n&gt;&gt;&gt; safe_log(torch.tensor([1e-4, 1e-5, 1e-6, 1e-8, 1e-9, 1e-10]))\ntensor([ -9.2103, -11.5129, -13.8155, -18.4207, -18.4207, -18.4207])\n</code></pre>"},{"location":"refs/root/","title":"karbonn","text":""},{"location":"refs/root/#karbonn","title":"karbonn","text":"<p>Root package.</p>"},{"location":"refs/root/#karbonn.Asinh","title":"karbonn.Asinh","text":"<p>             Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the inverse hyperbolic sine (arcsinh) of the elements.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Asinh\n&gt;&gt;&gt; m = Asinh()\n&gt;&gt;&gt; m\nAsinh()\n&gt;&gt;&gt; output = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; output\ntensor([[-0.8814,  0.0000,  0.8814],\n        [-1.4436,  1.4436,  2.0947]])\n</code></pre>"},{"location":"refs/root/#karbonn.BinaryFocalLoss","title":"karbonn.BinaryFocalLoss","text":"<p>             Bases: <code>Module</code></p> <p>Implementation of the binary Focal Loss.</p> <p>Based on \"Focal Loss for Dense Object Detection\" (https://arxiv.org/pdf/1708.02002.pdf)</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Module | dict</code> <p>The binary cross entropy layer or another equivalent layer. To be used as in the original paper, this loss should not use reducton as the reduction is done in this class.</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>.</p> <code>0.5</code> <code>gamma</code> <code>float</code> <p>The focusing parameter, which must be positive (<code>&gt;=0</code>).</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>The reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Target: <code>(*)</code>, same shape as the input.</li> <li>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code>(*)</code>, same   shape as input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import BinaryFocalLoss\n&gt;&gt;&gt; criterion = BinaryFocalLoss(nn.BCEWithLogitsLoss(reduction=\"none\"))\n&gt;&gt;&gt; input = torch.randn(3, 2, requires_grad=True)\n&gt;&gt;&gt; target = torch.rand(3, 2, requires_grad=False)\n&gt;&gt;&gt; output = criterion(input, target)\n&gt;&gt;&gt; output.backward()\n</code></pre>"},{"location":"refs/root/#karbonn.BinaryFocalLoss.forward","title":"karbonn.BinaryFocalLoss.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; Tensor\n</code></pre> <p>Compute the binary Focal Loss.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted probabilities or the un-normalized scores.</p> required <code>target</code> <code>Tensor</code> <p>The targets where <code>1</code> (resp. <code>0</code>) means a positive (resp. negative) example.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code> of type float: The loss value(s). The shape of the tensor depends on the reduction. If the reduction is <code>mean</code> or <code>sum</code>, the tensor has a single scalar value. If the reduction is <code>none</code>, the shape of the tensor is the same that the inputs.</p>"},{"location":"refs/root/#karbonn.Clamp","title":"karbonn.Clamp","text":"<p>             Bases: <code>Module</code></p> <p>Implement a module to clamp all elements in input into the range <code>[min, max]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float | None</code> <p>The lower-bound of the range to be clamped to. <code>None</code> means there is no minimum value.</p> <code>-1.0</code> <code>max</code> <code>float | None</code> <p>The upper-bound of the range to be clamped to. <code>None</code> means there is no maximum value.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Clamp\n&gt;&gt;&gt; m = Clamp(min=-1, max=2)\n&gt;&gt;&gt; m\nClamp(min=-1, max=2)\n&gt;&gt;&gt; m(torch.tensor([[-2.0, -1.0, 0.0], [1.0, 2.0, 3.0]]))\ntensor([[-1., -1.,  0.], [ 1.,  2.,  2.]])\n</code></pre>"},{"location":"refs/root/#karbonn.Exp","title":"karbonn.Exp","text":"<p>             Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the input.</p> <p>This module is equivalent to  <code>exp(input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Exp\n&gt;&gt;&gt; m = Exp()\n&gt;&gt;&gt; m\nExp()\n&gt;&gt;&gt; output = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; output\ntensor([[ 0.3679,  1.0000,  2.7183],\n        [ 0.1353,  7.3891, 54.5981]])\n</code></pre>"},{"location":"refs/root/#karbonn.ExpSin","title":"karbonn.ExpSin","text":"<p>             Bases: <code>BaseAlphaActivation</code></p> <p>Implement the ExpSin activation layer.</p> <p>Formula: <code>exp(-sin(alpha * x))</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import ExpSin\n&gt;&gt;&gt; m = ExpSin()\n&gt;&gt;&gt; m\nExpSin(num_parameters=1, learnable=True)\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[1.0000, 2.3198, 2.4826, 1.1516],\n        [0.4692, 0.3833, 0.7562, 1.9290]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/root/#karbonn.Expm1","title":"karbonn.Expm1","text":"<p>             Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the elements minus 1 of input.</p> <p>This module is equivalent to  <code>exp(input) - 1</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Expm1\n&gt;&gt;&gt; m = Expm1()\n&gt;&gt;&gt; m\nExpm1()\n&gt;&gt;&gt; output = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; output\ntensor([[-0.6321,  0.0000,  1.7183],\n        [-0.8647,  6.3891, 53.5981]])\n</code></pre>"},{"location":"refs/root/#karbonn.Gaussian","title":"karbonn.Gaussian","text":"<p>             Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Gaussian activation layer.</p> <p>Formula: <code>exp(-0.5 * x^2 / alpha^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Gaussian\n&gt;&gt;&gt; m = Gaussian()\n&gt;&gt;&gt; m\nGaussian(num_parameters=1, learnable=True)\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[1.0000e+00, 6.0653e-01, 1.3534e-01, 1.1109e-02],\n        [3.3546e-04, 3.7267e-06, 1.5230e-08, 2.2897e-11]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/root/#karbonn.Laplacian","title":"karbonn.Laplacian","text":"<p>             Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Laplacian activation layer.</p> <p>Formula: <code>exp(-|x| / alpha)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Laplacian\n&gt;&gt;&gt; m = Laplacian()\n&gt;&gt;&gt; m\nLaplacian(num_parameters=1, learnable=True)\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[1.0000e+00, 3.6788e-01, 1.3534e-01, 4.9787e-02],\n        [1.8316e-02, 6.7379e-03, 2.4788e-03, 9.1188e-04]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/root/#karbonn.Log","title":"karbonn.Log","text":"<p>             Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the natural logarithm of the input.</p> <p>This module is equivalent to  <code>log(input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Log\n&gt;&gt;&gt; m = Log()\n&gt;&gt;&gt; m\nLog()\n&gt;&gt;&gt; output = m(torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]))\n&gt;&gt;&gt; output\ntensor([[0.0000, 0.6931, 1.0986],\n        [1.3863, 1.6094, 1.7918]])\n</code></pre>"},{"location":"refs/root/#karbonn.Log1p","title":"karbonn.Log1p","text":"<p>             Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the natural logarithm of <code>(1 + input)</code>.</p> <p>This module is equivalent to  <code>log(1 + input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Log1p\n&gt;&gt;&gt; m = Log1p()\n&gt;&gt;&gt; m\nLog1p()\n&gt;&gt;&gt; output = m(torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]]))\n&gt;&gt;&gt; output\ntensor([[0.0000, 0.6931, 1.0986],\n        [1.3863, 1.6094, 1.7918]])\n</code></pre>"},{"location":"refs/root/#karbonn.MultiQuadratic","title":"karbonn.MultiQuadratic","text":"<p>             Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Multi Quadratic activation layer.</p> <p>Formula: <code>1 / sqrt(1 + (alpha * x)^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import MultiQuadratic\n&gt;&gt;&gt; m = MultiQuadratic()\n&gt;&gt;&gt; m\nMultiQuadratic(num_parameters=1, learnable=True)\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[1.0000, 0.7071, 0.4472, 0.3162],\n        [0.2425, 0.1961, 0.1644, 0.1414]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/root/#karbonn.Quadratic","title":"karbonn.Quadratic","text":"<p>             Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Quadratic activation layer.</p> <p>Formula: <code>1 / (1 + (alpha * x)^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Quadratic\n&gt;&gt;&gt; m = Quadratic()\n&gt;&gt;&gt; m\nQuadratic(num_parameters=1, learnable=True)\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[1.0000, 0.5000, 0.2000, 0.1000],\n        [0.0588, 0.0385, 0.0270, 0.0200]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/root/#karbonn.ReLUn","title":"karbonn.ReLUn","text":"<p>             Bases: <code>Module</code></p> <p>Implements the ReLU-n module.</p> <p>The ReLU-n equation is: <code>ReLUn(x, n)=min(max(0,x),n)</code></p> <p>Parameters:</p> Name Type Description Default <code>max</code> <code>float</code> <p>The maximum value a.k.a. <code>n</code> in the equation above.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import ReLUn\n&gt;&gt;&gt; m = ReLUn(max=5)\n&gt;&gt;&gt; m\nReLUn(max=5.0)\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[0., 1., 2., 3.],\n        [4., 5., 5., 5.]])\n</code></pre>"},{"location":"refs/root/#karbonn.SafeExp","title":"karbonn.SafeExp","text":"<p>             Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the elements.</p> <p>The values that are higher than the specified minimum value are set to this maximum value. Using a not too large positive value leads to an output tensor without Inf.</p> <p>Parameters:</p> Name Type Description Default <code>max</code> <code>float</code> <p>The maximum value before to compute the exponential.</p> <code>20.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import SafeExp\n&gt;&gt;&gt; m = SafeExp()\n&gt;&gt;&gt; m\nSafeExp(max=20.0)\n&gt;&gt;&gt; output = m(torch.tensor([[0.01, 0.1, 1.0], [10.0, 100.0, 1000.0]]))\n&gt;&gt;&gt; output\ntensor([[1.0101e+00, 1.1052e+00, 2.7183e+00],\n        [2.2026e+04, 4.8517e+08, 4.8517e+08]])\n</code></pre>"},{"location":"refs/root/#karbonn.SafeLog","title":"karbonn.SafeLog","text":"<p>             Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the logarithm natural of the elements.</p> <p>The values that are lower than the specified minimum value are set to this minimum value. Using a small positive value leads to an output tensor without NaN or Inf.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float</code> <p>The minimum value before to compute the logarithm natural.</p> <code>1e-08</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import SafeLog\n&gt;&gt;&gt; m = SafeLog()\n&gt;&gt;&gt; m\nSafeLog(min=1e-08)\n&gt;&gt;&gt; output = m(torch.tensor([[1e-4, 1e-5, 1e-6], [1e-8, 1e-9, 1e-10]]))\n&gt;&gt;&gt; output\ntensor([[ -9.2103, -11.5129, -13.8155],\n        [-18.4207, -18.4207, -18.4207]])\n</code></pre>"},{"location":"refs/root/#karbonn.Sin","title":"karbonn.Sin","text":"<p>             Bases: <code>BaseAlphaActivation</code></p> <p>Implement the sine activation layer.</p> <p>Formula: <code>sin(alpha * x)</code></p> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Sin\n&gt;&gt;&gt; m = Sin()\n&gt;&gt;&gt; m\nSin(num_parameters=1, learnable=True)\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[ 0.0000,  0.8415,  0.9093,  0.1411],\n        [-0.7568, -0.9589, -0.2794,  0.6570]], grad_fn=&lt;SinBackward0&gt;)\n</code></pre>"},{"location":"refs/root/#karbonn.Sinh","title":"karbonn.Sinh","text":"<p>             Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the hyperbolic sine (sinh) of the elements.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Sinh\n&gt;&gt;&gt; m = Sinh()\n&gt;&gt;&gt; m\nSinh()\n&gt;&gt;&gt; output = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; output\ntensor([[-1.1752,  0.0000,  1.1752],\n        [-3.6269,  3.6269, 27.2899]])\n</code></pre>"},{"location":"refs/root/#karbonn.Snake","title":"karbonn.Snake","text":"<p>             Bases: <code>Module</code></p> <p>Implements the Snake activation layer.</p> <p>Snake was proposed in the following paper:</p> <pre><code>Neural Networks Fail to Learn Periodic Functions and How to Fix It.\nZiyin L., Hartwig T., Ueda M.\nNeurIPS, 2020. (http://arxiv.org/pdf/2006.08195)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>float</code> <p>The frequency.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import Snake\n&gt;&gt;&gt; m = Snake()\n&gt;&gt;&gt; m\nSnake(frequency=1.0)\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[0.0000, 1.7081, 2.8268, 3.0199],\n        [4.5728, 5.9195, 6.0781, 7.4316]])\n</code></pre>"},{"location":"refs/root/#karbonn.SquaredReLU","title":"karbonn.SquaredReLU","text":"<p>             Bases: <code>Module</code></p> <p>Implements the Squared ReLU.</p> <p>Squared ReLU is defined in the following paper:</p> <pre><code>Primer: Searching for Efficient Transformers for Language Modeling.\nSo DR., Ma\u0144ke W., Liu H., Dai Z., Shazeer N., Le QV.\nNeurIPS, 2021. (https://arxiv.org/pdf/2109.08668.pdf)\n</code></pre> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn import SquaredReLU\n&gt;&gt;&gt; m = SquaredReLU()\n&gt;&gt;&gt; m\nSquaredReLU()\n&gt;&gt;&gt; output = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; output\ntensor([[ 0.,  1.,  4.,  9.],\n        [16., 25., 36., 49.]])\n</code></pre>"},{"location":"refs/root/#karbonn.binary_focal_loss","title":"karbonn.binary_focal_loss","text":"<pre><code>binary_focal_loss(\n    alpha: float = 0.5,\n    gamma: float = 2.0,\n    reduction: str = \"mean\",\n    logits: bool = False,\n) -&gt; BinaryFocalLoss\n</code></pre> <p>Return an instantiated binary focal loss with a binary cross entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>.</p> <code>0.5</code> <code>gamma</code> <code>float</code> <p>The focusing parameter, which must be positive (<code>&gt;=0</code>).</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>The reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>logits</code> <code>bool</code> <p>If <code>True</code>, the <code>torch.nn.BCEWithLogitsLoss</code> is used, otherwise <code>torch.nn.BCELoss</code> is used.</p> <code>False</code> <p>Returns:</p> Type Description <code>BinaryFocalLoss</code> <p>The instantiated binary focal loss.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn import binary_focal_loss\n&gt;&gt;&gt; criterion = binary_focal_loss()\n&gt;&gt;&gt; criterion\nBinaryFocalLoss(\n  alpha=0.5, gamma=2.0\n  (loss): BCELoss()\n)\n&gt;&gt;&gt; criterion = binary_focal_loss(logits=True)\n&gt;&gt;&gt; criterion\nBinaryFocalLoss(\n  alpha=0.5, gamma=2.0\n  (loss): BCEWithLogitsLoss()\n)\n</code></pre>"},{"location":"refs/root/#karbonn.freeze_module","title":"karbonn.freeze_module","text":"<pre><code>freeze_module(module: Module) -&gt; None\n</code></pre> <p>Freeze the parameters of the given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to freeze.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import freeze_module\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; for name, param in module.named_parameters():\n...     print(name, param.requires_grad)\n...\nweight False\nbias False\n</code></pre>"},{"location":"refs/root/#karbonn.get_module_device","title":"karbonn.get_module_device","text":"<pre><code>get_module_device(module: Module) -&gt; device\n</code></pre> <p>Get the device used by this module.</p> <p>This function assumes the module uses a single device. If the module uses several devices, you should use <code>get_module_devices</code>. It returns <code>torch.device('cpu')</code> if the model does not have parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>device</code> <p>The device</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import get_module_device\n&gt;&gt;&gt; get_module_device(torch.nn.Linear(4, 6))\ndevice(type='cpu')\n</code></pre>"},{"location":"refs/root/#karbonn.get_module_devices","title":"karbonn.get_module_devices","text":"<pre><code>get_module_devices(module: Module) -&gt; tuple[device, ...]\n</code></pre> <p>Get the devices used in a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>tuple[device, ...]</code> <p>The tuple of <code>torch.device</code>s used in the module.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import get_module_devices\n&gt;&gt;&gt; get_module_devices(torch.nn.Linear(4, 6))\n(device(type='cpu'),)\n</code></pre>"},{"location":"refs/root/#karbonn.has_learnable_parameters","title":"karbonn.has_learnable_parameters","text":"<pre><code>has_learnable_parameters(module: Module) -&gt; bool\n</code></pre> <p>Indicate if the module has learnable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module has at least one learnable parameter, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import has_learnable_parameters, freeze_module\n&gt;&gt;&gt; has_learnable_parameters(torch.nn.Linear(4, 6))\nTrue\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; has_learnable_parameters(module)\nFalse\n&gt;&gt;&gt; has_learnable_parameters(torch.nn.Identity())\nFalse\n</code></pre>"},{"location":"refs/root/#karbonn.has_parameters","title":"karbonn.has_parameters","text":"<pre><code>has_parameters(module: Module) -&gt; bool\n</code></pre> <p>Indicate if the module has parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module has at least one parameter, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import has_parameters\n&gt;&gt;&gt; has_parameters(torch.nn.Linear(4, 6))\nTrue\n&gt;&gt;&gt; has_parameters(torch.nn.Identity())\nFalse\n</code></pre>"},{"location":"refs/root/#karbonn.is_loss_decreasing","title":"karbonn.is_loss_decreasing","text":"<pre><code>is_loss_decreasing(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    optimizer: Optimizer,\n    feature: Tensor,\n    target: Tensor,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to update the weights of the model.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing\n&gt;&gt;&gt; module = nn.Linear(4, 2)\n&gt;&gt;&gt; is_loss_decreasing(\n...     module=module,\n...     criterion=nn.MSELoss(),\n...     optimizer=SGD(module.parameters(), lr=0.01),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n... )\nTrue\n</code></pre>"},{"location":"refs/root/#karbonn.is_loss_decreasing_with_adam","title":"karbonn.is_loss_decreasing_with_adam","text":"<pre><code>is_loss_decreasing_with_adam(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    feature: Tensor,\n    target: Tensor,\n    lr: float = 0.0003,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>The module is trained with the Adam optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.0003</code> <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing_with_adam\n&gt;&gt;&gt; is_loss_decreasing_with_adam(\n...     module=nn.Linear(4, 2),\n...     criterion=nn.MSELoss(),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n...     lr=0.0003,\n... )\nTrue\n</code></pre>"},{"location":"refs/root/#karbonn.is_loss_decreasing_with_sgd","title":"karbonn.is_loss_decreasing_with_sgd","text":"<pre><code>is_loss_decreasing_with_sgd(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    feature: Tensor,\n    target: Tensor,\n    lr: float = 0.01,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>The module is trained with the <code>torch.optim.SGD</code> optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.01</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing_with_adam\n&gt;&gt;&gt; is_loss_decreasing_with_adam(\n...     module=nn.Linear(4, 2),\n...     criterion=nn.MSELoss(),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n...     lr=0.01,\n... )\nTrue\n</code></pre>"},{"location":"refs/root/#karbonn.is_module_config","title":"karbonn.is_module_config","text":"<pre><code>is_module_config(config: dict) -&gt; bool\n</code></pre> <p>Indicate if the input configuration is a configuration for a <code>torch.nn.Module</code>.</p> <p>This function only checks if the value of the key  <code>_target_</code> is valid. It does not check the other values. If <code>_target_</code> indicates a function, the returned type hint is used to check the class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the input configuration is a configuration for a <code>torch.nn.Module</code> object, otherwise <code>False</code>..</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn import is_module_config\n&gt;&gt;&gt; is_module_config({\"_target_\": \"torch.nn.Identity\"})\nTrue\n</code></pre>"},{"location":"refs/root/#karbonn.is_module_on_device","title":"karbonn.is_module_on_device","text":"<pre><code>is_module_on_device(module: Module, device: device) -&gt; bool\n</code></pre> <p>Indicate if all the parameters of a module are on the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <code>device</code> <code>device</code> <p>The device.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all the parameters of the module are on the specified device, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import is_module_on_device\n&gt;&gt;&gt; is_module_on_device(torch.nn.Linear(4, 6), torch.device(\"cpu\"))\nTrue\n</code></pre>"},{"location":"refs/root/#karbonn.module_mode","title":"karbonn.module_mode","text":"<pre><code>module_mode(module: Module) -&gt; Generator[None, None, None]\n</code></pre> <p>Implement a context manager that restores the mode (train or eval) of every submodule individually.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to restore the mode.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import module_mode\n&gt;&gt;&gt; module = torch.nn.ModuleDict(\n...     {\"module1\": torch.nn.Linear(4, 6), \"module2\": torch.nn.Linear(2, 4).eval()}\n... )\n&gt;&gt;&gt; print(module[\"module1\"].training, module[\"module2\"].training)\nTrue False\n&gt;&gt;&gt; with module_mode(module):\n...     module.eval()\n...     print(module[\"module1\"].training, module[\"module2\"].training)\n...\nModuleDict(\n  (module1): Linear(in_features=4, out_features=6, bias=True)\n  (module2): Linear(in_features=2, out_features=4, bias=True)\n)\nFalse False\n&gt;&gt;&gt; print(module[\"module1\"].training, module[\"module2\"].training)\nTrue False\n</code></pre>"},{"location":"refs/root/#karbonn.num_learnable_parameters","title":"karbonn.num_learnable_parameters","text":"<pre><code>num_learnable_parameters(module: Module) -&gt; int\n</code></pre> <p>Return the number of learnable parameters in the module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to compute the number of learnable parameters.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of learnable parameters.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import num_learnable_parameters\n&gt;&gt;&gt; num_learnable_parameters(torch.nn.Linear(4, 6))\n30\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; num_learnable_parameters(module)\n0\n&gt;&gt;&gt; num_learnable_parameters(torch.nn.Identity())\n0\n</code></pre>"},{"location":"refs/root/#karbonn.num_parameters","title":"karbonn.num_parameters","text":"<pre><code>num_parameters(module: Module) -&gt; int\n</code></pre> <p>Return the number of parameters in the module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to compute the number of parameters.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of parameters.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import num_parameters\n&gt;&gt;&gt; num_parameters(torch.nn.Linear(4, 6))\n30\n&gt;&gt;&gt; num_parameters(torch.nn.Identity())\n0\n</code></pre>"},{"location":"refs/root/#karbonn.setup_module","title":"karbonn.setup_module","text":"<pre><code>setup_module(module: Module | dict) -&gt; Module\n</code></pre> <p>Set up a <code>torch.nn.Module</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module | dict</code> <p>The module or its configuration.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The instantiated <code>torch.nn.Module</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn import setup_module\n&gt;&gt;&gt; linear = setup_module(\n...     {\"_target_\": \"torch.nn.Linear\", \"in_features\": 4, \"out_features\": 6}\n... )\n&gt;&gt;&gt; linear\nLinear(in_features=4, out_features=6, bias=True)\n</code></pre>"},{"location":"refs/root/#karbonn.top_module_mode","title":"karbonn.top_module_mode","text":"<pre><code>top_module_mode(\n    module: Module,\n) -&gt; Generator[None, None, None]\n</code></pre> <p>Implement a context manager that restores the mode (train or eval) of a given module.</p> <p>This context manager only restores the mode at the top-level.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to restore the mode.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import top_module_mode\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; print(module.training)\nTrue\n&gt;&gt;&gt; with top_module_mode(module):\n...     module.eval()\n...     print(module.training)\n...\nLinear(in_features=4, out_features=6, bias=True)\nFalse\n&gt;&gt;&gt; print(module.training)\nTrue\n</code></pre>"},{"location":"refs/root/#karbonn.unfreeze_module","title":"karbonn.unfreeze_module","text":"<pre><code>unfreeze_module(module: Module) -&gt; None\n</code></pre> <p>Unfreeze the parameters of the given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to unfreeze.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import unfreeze_module\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; unfreeze_module(module)\n&gt;&gt;&gt; for name, param in module.named_parameters():\n...     print(name, param.requires_grad)\n...\nweight True\nbias True\n</code></pre>"},{"location":"refs/utils/","title":"karbonn.utils","text":""},{"location":"refs/utils/#karbonn.utils","title":"karbonn.utils","text":"<p>Contain utility functions.</p>"},{"location":"refs/utils/#karbonn.utils.freeze_module","title":"karbonn.utils.freeze_module","text":"<pre><code>freeze_module(module: Module) -&gt; None\n</code></pre> <p>Freeze the parameters of the given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to freeze.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import freeze_module\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; for name, param in module.named_parameters():\n...     print(name, param.requires_grad)\n...\nweight False\nbias False\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.get_module_device","title":"karbonn.utils.get_module_device","text":"<pre><code>get_module_device(module: Module) -&gt; device\n</code></pre> <p>Get the device used by this module.</p> <p>This function assumes the module uses a single device. If the module uses several devices, you should use <code>get_module_devices</code>. It returns <code>torch.device('cpu')</code> if the model does not have parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>device</code> <p>The device</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import get_module_device\n&gt;&gt;&gt; get_module_device(torch.nn.Linear(4, 6))\ndevice(type='cpu')\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.get_module_devices","title":"karbonn.utils.get_module_devices","text":"<pre><code>get_module_devices(module: Module) -&gt; tuple[device, ...]\n</code></pre> <p>Get the devices used in a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>tuple[device, ...]</code> <p>The tuple of <code>torch.device</code>s used in the module.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import get_module_devices\n&gt;&gt;&gt; get_module_devices(torch.nn.Linear(4, 6))\n(device(type='cpu'),)\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.has_learnable_parameters","title":"karbonn.utils.has_learnable_parameters","text":"<pre><code>has_learnable_parameters(module: Module) -&gt; bool\n</code></pre> <p>Indicate if the module has learnable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module has at least one learnable parameter, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import has_learnable_parameters, freeze_module\n&gt;&gt;&gt; has_learnable_parameters(torch.nn.Linear(4, 6))\nTrue\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; has_learnable_parameters(module)\nFalse\n&gt;&gt;&gt; has_learnable_parameters(torch.nn.Identity())\nFalse\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.has_parameters","title":"karbonn.utils.has_parameters","text":"<pre><code>has_parameters(module: Module) -&gt; bool\n</code></pre> <p>Indicate if the module has parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module has at least one parameter, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import has_parameters\n&gt;&gt;&gt; has_parameters(torch.nn.Linear(4, 6))\nTrue\n&gt;&gt;&gt; has_parameters(torch.nn.Identity())\nFalse\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_loss_decreasing","title":"karbonn.utils.is_loss_decreasing","text":"<pre><code>is_loss_decreasing(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    optimizer: Optimizer,\n    feature: Tensor,\n    target: Tensor,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to update the weights of the model.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing\n&gt;&gt;&gt; module = nn.Linear(4, 2)\n&gt;&gt;&gt; is_loss_decreasing(\n...     module=module,\n...     criterion=nn.MSELoss(),\n...     optimizer=SGD(module.parameters(), lr=0.01),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_loss_decreasing_with_adam","title":"karbonn.utils.is_loss_decreasing_with_adam","text":"<pre><code>is_loss_decreasing_with_adam(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    feature: Tensor,\n    target: Tensor,\n    lr: float = 0.0003,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>The module is trained with the Adam optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.0003</code> <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing_with_adam\n&gt;&gt;&gt; is_loss_decreasing_with_adam(\n...     module=nn.Linear(4, 2),\n...     criterion=nn.MSELoss(),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n...     lr=0.0003,\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_loss_decreasing_with_sgd","title":"karbonn.utils.is_loss_decreasing_with_sgd","text":"<pre><code>is_loss_decreasing_with_sgd(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    feature: Tensor,\n    target: Tensor,\n    lr: float = 0.01,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>The module is trained with the <code>torch.optim.SGD</code> optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.01</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing_with_adam\n&gt;&gt;&gt; is_loss_decreasing_with_adam(\n...     module=nn.Linear(4, 2),\n...     criterion=nn.MSELoss(),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n...     lr=0.01,\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_module_config","title":"karbonn.utils.is_module_config","text":"<pre><code>is_module_config(config: dict) -&gt; bool\n</code></pre> <p>Indicate if the input configuration is a configuration for a <code>torch.nn.Module</code>.</p> <p>This function only checks if the value of the key  <code>_target_</code> is valid. It does not check the other values. If <code>_target_</code> indicates a function, the returned type hint is used to check the class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the input configuration is a configuration for a <code>torch.nn.Module</code> object, otherwise <code>False</code>..</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn import is_module_config\n&gt;&gt;&gt; is_module_config({\"_target_\": \"torch.nn.Identity\"})\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_module_on_device","title":"karbonn.utils.is_module_on_device","text":"<pre><code>is_module_on_device(module: Module, device: device) -&gt; bool\n</code></pre> <p>Indicate if all the parameters of a module are on the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <code>device</code> <code>device</code> <p>The device.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all the parameters of the module are on the specified device, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import is_module_on_device\n&gt;&gt;&gt; is_module_on_device(torch.nn.Linear(4, 6), torch.device(\"cpu\"))\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.module_mode","title":"karbonn.utils.module_mode","text":"<pre><code>module_mode(module: Module) -&gt; Generator[None, None, None]\n</code></pre> <p>Implement a context manager that restores the mode (train or eval) of every submodule individually.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to restore the mode.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import module_mode\n&gt;&gt;&gt; module = torch.nn.ModuleDict(\n...     {\"module1\": torch.nn.Linear(4, 6), \"module2\": torch.nn.Linear(2, 4).eval()}\n... )\n&gt;&gt;&gt; print(module[\"module1\"].training, module[\"module2\"].training)\nTrue False\n&gt;&gt;&gt; with module_mode(module):\n...     module.eval()\n...     print(module[\"module1\"].training, module[\"module2\"].training)\n...\nModuleDict(\n  (module1): Linear(in_features=4, out_features=6, bias=True)\n  (module2): Linear(in_features=2, out_features=4, bias=True)\n)\nFalse False\n&gt;&gt;&gt; print(module[\"module1\"].training, module[\"module2\"].training)\nTrue False\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.num_learnable_parameters","title":"karbonn.utils.num_learnable_parameters","text":"<pre><code>num_learnable_parameters(module: Module) -&gt; int\n</code></pre> <p>Return the number of learnable parameters in the module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to compute the number of learnable parameters.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of learnable parameters.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import num_learnable_parameters\n&gt;&gt;&gt; num_learnable_parameters(torch.nn.Linear(4, 6))\n30\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; num_learnable_parameters(module)\n0\n&gt;&gt;&gt; num_learnable_parameters(torch.nn.Identity())\n0\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.num_parameters","title":"karbonn.utils.num_parameters","text":"<pre><code>num_parameters(module: Module) -&gt; int\n</code></pre> <p>Return the number of parameters in the module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to compute the number of parameters.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of parameters.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import num_parameters\n&gt;&gt;&gt; num_parameters(torch.nn.Linear(4, 6))\n30\n&gt;&gt;&gt; num_parameters(torch.nn.Identity())\n0\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.setup_module","title":"karbonn.utils.setup_module","text":"<pre><code>setup_module(module: Module | dict) -&gt; Module\n</code></pre> <p>Set up a <code>torch.nn.Module</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module | dict</code> <p>The module or its configuration.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The instantiated <code>torch.nn.Module</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn import setup_module\n&gt;&gt;&gt; linear = setup_module(\n...     {\"_target_\": \"torch.nn.Linear\", \"in_features\": 4, \"out_features\": 6}\n... )\n&gt;&gt;&gt; linear\nLinear(in_features=4, out_features=6, bias=True)\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.top_module_mode","title":"karbonn.utils.top_module_mode","text":"<pre><code>top_module_mode(\n    module: Module,\n) -&gt; Generator[None, None, None]\n</code></pre> <p>Implement a context manager that restores the mode (train or eval) of a given module.</p> <p>This context manager only restores the mode at the top-level.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to restore the mode.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import top_module_mode\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; print(module.training)\nTrue\n&gt;&gt;&gt; with top_module_mode(module):\n...     module.eval()\n...     print(module.training)\n...\nLinear(in_features=4, out_features=6, bias=True)\nFalse\n&gt;&gt;&gt; print(module.training)\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.unfreeze_module","title":"karbonn.utils.unfreeze_module","text":"<pre><code>unfreeze_module(module: Module) -&gt; None\n</code></pre> <p>Unfreeze the parameters of the given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to unfreeze.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import unfreeze_module\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; unfreeze_module(module)\n&gt;&gt;&gt; for name, param in module.named_parameters():\n...     print(name, param.requires_grad)\n...\nweight True\nbias True\n</code></pre>"}]}