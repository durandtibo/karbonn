{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>TODO</p>"},{"location":"#motivation","title":"Motivation","text":"<p>TODO</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>karbonn</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>karbonn</code> to a new version will possibly break any code that was using the old version of <code>karbonn</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>karbonn</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install karbonn\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>karbonn</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'karbonn[all]'\n</code></pre> <p>This command also installed NumPy and PyTorch. It is also possible to install the optional packages manually or to select the packages to install. In the following example, only NumPy is installed:</p> <pre><code>pip install karbonn numpy\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>karbonn</code> from source, you can follow the steps below. First, you will need to install <code>poetry</code>. <code>poetry</code> is used to manage and install the dependencies. If <code>poetry</code> is already installed on your machine, you can skip this step. There are several ways to install <code>poetry</code> so you can use the one that you prefer. You can check the <code>poetry</code> installation by running the following command:</p> <pre><code>poetry --version\n</code></pre> <p>Then, you can clone the git repository:</p> <pre><code>git clone git@github.com:durandtibo/karbonn.git\n</code></pre> <p>It is recommended to create a Python 3.8+ virtual environment. This step is optional so you can skip it. To create a virtual environment, you can use the following command:</p> <pre><code>make conda\n</code></pre> <p>It automatically creates a conda virtual environment. When the virtual environment is created, you can activate it with the following command:</p> <pre><code>conda activate karbonn\n</code></pre> <p>This example uses <code>conda</code> to create a virtual environment, but you can use other tools or configurations. Then, you should install the required package to use <code>karbonn</code> with the following command:</p> <pre><code>make install\n</code></pre> <p>This command will install all the required packages. You can also use this command to update the required packages. This command will check if there is a more recent package available and will install it. Finally, you can test the installation with the following command:</p> <pre><code>make unit-test-cov\n</code></pre>"},{"location":"refs/distributed/","title":"karbonn.distributed","text":""},{"location":"refs/functional/","title":"karbonn.functional","text":""},{"location":"refs/functional/#activations","title":"Activations","text":""},{"location":"refs/functional/#karbonn.functional.safe_exp","title":"karbonn.functional.safe_exp","text":"<pre><code>safe_exp(input: Tensor, max: float = 20.0) -&gt; Tensor\n</code></pre> <p>Compute safely the exponential of the elements.</p> <p>The values that are higher than the specified minimum value are set to this maximum value. Using a not too large positive value leads to an output tensor without Inf.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>max</code> <code>float</code> <p>The maximum value.</p> <code>20.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with the exponential of the elements.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import safe_exp\n&gt;&gt;&gt; output = safe_exp(torch.tensor([1.0, 10.0, 100.0, 1000.0]))\n&gt;&gt;&gt; output\ntensor([2.7183e+00, 2.2026e+04, 4.8517e+08, 4.8517e+08])\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.safe_log","title":"karbonn.functional.safe_log","text":"<pre><code>safe_log(input: Tensor, min: float = 1e-08) -&gt; Tensor\n</code></pre> <p>Compute safely the logarithm natural logarithm of the elements.</p> <p>The values that are lower than the specified minimum value are set to this minimum value. Using a small positive value leads to an output tensor without NaN or Inf.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor.</p> required <code>min</code> <code>float</code> <p>The minimum value.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor with the natural logarithm of the elements.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import safe_log\n&gt;&gt;&gt; safe_log(torch.tensor([1e-4, 1e-5, 1e-6, 1e-8, 1e-9, 1e-10]))\ntensor([ -9.2103, -11.5129, -13.8155, -18.4207, -18.4207, -18.4207])\n</code></pre>"},{"location":"refs/functional/#loss-functions","title":"Loss functions","text":""},{"location":"refs/functional/#karbonn.functional.asinh_mse_loss","title":"karbonn.functional.asinh_mse_loss","text":"<pre><code>asinh_mse_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the mean squared error (MSE) on the inverse hyperbolic sine (asinh) transformed predictions and targets.</p> <p>It is a generalization of mean squared logarithmic error (MSLE) that works for real values. The <code>asinh</code> transformation is used instead of <code>log1p</code> because <code>asinh</code> works on negative values.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean squared error (MSE) on the inverse hyperbolic sine (asinh) transformed predictions and targets. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import asinh_mse_loss\n&gt;&gt;&gt; loss = asinh_mse_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MseLossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.asinh_smooth_l1_loss","title":"karbonn.functional.asinh_smooth_l1_loss","text":"<pre><code>asinh_smooth_l1_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    beta: float = 1.0,\n) -&gt; Tensor\n</code></pre> <p>Compute the smooth L1 loss on the inverse hyperbolic sine (asinh) transformed predictions and targets.</p> <p>It is a generalization of mean squared logarithmic error (MSLE) that works for real values. The <code>asinh</code> transformation is used instead of <code>log1p</code> because <code>asinh</code> works on negative values.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>beta</code> <code>float</code> <p>The threshold at which to change between L1 and L2 loss. The value must be non-negative.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The smooth L1 loss on the inverse hyperbolic sine (asinh) transformed predictions and targets. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import asinh_smooth_l1_loss\n&gt;&gt;&gt; loss = asinh_smooth_l1_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;SmoothL1LossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.general_robust_regression_loss","title":"karbonn.functional.general_robust_regression_loss","text":"<pre><code>general_robust_regression_loss(\n    prediction: Tensor,\n    target: Tensor,\n    alpha: float = 2.0,\n    scale: float = 1.0,\n    max: float | None = None,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the general robust regression loss a.k.a. Barron robust loss.</p> <p>Based on the paper:</p> <pre><code>A General and Adaptive Robust Loss Function\nJonathan T. Barron\nCVPR 2019 (https://arxiv.org/abs/1701.03077)\n</code></pre> Note <p>The \"adaptative\" part of the loss is not implemented in this     function.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>alpha</code> <code>float</code> <p>The shape parameter that controls the robustness of the loss.</p> <code>2.0</code> <code>scale</code> <code>float</code> <p>The scale parameter that controls the size of the loss's quadratic bowl near 0.</p> <code>1.0</code> <code>max</code> <code>float | None</code> <p>The max value to clip the loss before to compute the reduction. <code>None</code> means no clipping is used.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss. The shape of the tensor depends on the reduction strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import general_robust_regression_loss\n&gt;&gt;&gt; loss = general_robust_regression_loss(\n...     torch.randn(2, 4, requires_grad=True), torch.randn(2, 4)\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.log_cosh_loss","title":"karbonn.functional.log_cosh_loss","text":"<pre><code>log_cosh_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n    scale: float = 1.0,\n) -&gt; Tensor\n</code></pre> <p>Compute the logarithm of the hyperbolic cosine of the prediction error.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>scale</code> <code>float</code> <p>The scale factor.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The logarithm of the hyperbolic cosine of the prediction error.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import log_cosh_loss\n&gt;&gt;&gt; loss = log_cosh_loss(torch.randn(3, 5, requires_grad=True), torch.randn(3, 5))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.msle_loss","title":"karbonn.functional.msle_loss","text":"<pre><code>msle_loss(\n    prediction: Tensor,\n    target: Tensor,\n    reduction: str = \"mean\",\n) -&gt; Tensor\n</code></pre> <p>Compute the mean squared error (MSE) on the logarithmic transformed predictions and targets.</p> <p>This loss is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this loss penalizes an under-predicted estimate greater than an over-predicted estimate.</p> <p>Note: this loss only works with positive values (0 included).</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The mean squared logarithmic error. The shape of the tensor depends on the reduction strategy.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import msle_loss\n&gt;&gt;&gt; loss = msle_loss(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MseLossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.reduce_loss","title":"karbonn.functional.reduce_loss","text":"<pre><code>reduce_loss(tensor: Tensor, reduction: str) -&gt; Tensor\n</code></pre> <p>Return the reduced loss.</p> <p>This function is designed to be used with loss functions.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The input tensor to reduce.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The reduced tensor. The shape of the tensor depends on the reduction strategy.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import reduce_loss\n&gt;&gt;&gt; tensor = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]])\n&gt;&gt;&gt; reduce_loss(tensor, \"none\")\ntensor([[0., 1., 2.],\n        [3., 4., 5.]])\n&gt;&gt;&gt; reduce_loss(tensor, \"sum\")\ntensor(15.)\n&gt;&gt;&gt; reduce_loss(tensor, \"mean\")\ntensor(2.5000)\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.relative_loss","title":"karbonn.functional.relative_loss","text":"<pre><code>relative_loss(\n    loss: Tensor,\n    indicator: Tensor,\n    reduction: str = \"mean\",\n    eps: float = 1e-08,\n) -&gt; Tensor\n</code></pre> <p>Compute the relative loss.</p> <p>The indicators are designed based on https://en.wikipedia.org/wiki/Relative_change#Indicators_of_relative_change.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Tensor</code> <p>The loss values. The tensor must have the same shape as the target.</p> required <code>indicator</code> <code>Tensor</code> <p>The indicator values.</p> required <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the indicator is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed relative loss.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the loss and indicator shapes do not match.</p> <code>ValueError</code> <p>if the reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import relative_loss\n&gt;&gt;&gt; prediction = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(3, 5)\n&gt;&gt;&gt; loss = relative_loss(\n...     loss=torch.nn.functional.mse_loss(prediction, target, reduction=\"none\"),\n...     indicator=classical_relative_indicator(prediction, target),\n... )\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/functional/#relative-loss-indicators","title":"Relative loss indicators","text":""},{"location":"refs/functional/#karbonn.functional.loss.arithmetical_mean_indicator","title":"karbonn.functional.loss.arithmetical_mean_indicator","text":"<pre><code>arithmetical_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the arithmetical mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional.loss import arithmetical_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = arithmetical_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[1.0000, 1.0000, 0.5000],\n        [3.0000, 3.0000, 1.0000]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.loss.classical_relative_indicator","title":"karbonn.functional.loss.classical_relative_indicator","text":"<pre><code>classical_relative_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the classical relative change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional.loss import classical_relative_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = classical_relative_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[2., 1., 0.],\n        [3., 5., 1.]])\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.loss.geometric_mean_indicator","title":"karbonn.functional.loss.geometric_mean_indicator","text":"<pre><code>geometric_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the geometric mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional.loss import geometric_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = geometric_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0.0000, 1.0000, 0.0000],\n        [3.0000, 2.2361, 1.0000]], grad_fn=&lt;SqrtBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.loss.maximum_mean_indicator","title":"karbonn.functional.loss.maximum_mean_indicator","text":"<pre><code>maximum_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the maximum mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional.loss import maximum_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = maximum_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[2., 1., 1.],\n        [3., 5., 1.]], grad_fn=&lt;MaximumBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.loss.minimum_mean_indicator","title":"karbonn.functional.loss.minimum_mean_indicator","text":"<pre><code>minimum_mean_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the minimum mean change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional.loss import minimum_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = minimum_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0., 1., 0.],\n        [3., 1., 1.]], grad_fn=&lt;MinimumBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.loss.moment_mean_indicator","title":"karbonn.functional.loss.moment_mean_indicator","text":"<pre><code>moment_mean_indicator(\n    prediction: Tensor, target: Tensor, k: int = 1\n) -&gt; Tensor\n</code></pre> <p>Return the moment mean change of order k.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <code>k</code> <code>int</code> <p>The order.</p> <code>1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional.loss import moment_mean_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = moment_mean_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[1.0000, 1.0000, 0.5000],\n        [3.0000, 3.0000, 1.0000]], grad_fn=&lt;PowBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.loss.reversed_relative_indicator","title":"karbonn.functional.loss.reversed_relative_indicator","text":"<pre><code>reversed_relative_indicator(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Return the reversed relative change.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The indicator values.</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional.loss import reversed_relative_indicator\n&gt;&gt;&gt; prediction = torch.tensor([[0.0, 1.0, -1.0], [3.0, 1.0, -1.0]], requires_grad=True)\n&gt;&gt;&gt; target = torch.tensor([[-2.0, 1.0, 0.0], [-3.0, 5.0, -1.0]])\n&gt;&gt;&gt; indicator = reversed_relative_indicator(prediction, target)\n&gt;&gt;&gt; indicator\ntensor([[0., 1., 1.],\n        [3., 1., 1.]], grad_fn=&lt;AbsBackward0&gt;)\n</code></pre>"},{"location":"refs/functional/#errors","title":"Errors","text":""},{"location":"refs/functional/#karbonn.functional.absolute_error","title":"karbonn.functional.absolute_error","text":"<pre><code>absolute_error(\n    prediction: Tensor, target: Tensor\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise absolute error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import absolute_error\n&gt;&gt;&gt; absolute_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 1.],\n        [1., 0.]])\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.absolute_relative_error","title":"karbonn.functional.absolute_relative_error","text":"<pre><code>absolute_relative_error(\n    prediction: Tensor, target: Tensor, eps: float = 1e-08\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise absolute relative error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The absolute relative error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import absolute_relative_error\n&gt;&gt;&gt; absolute_relative_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 1.],\n        [1., 0.]])\n</code></pre>"},{"location":"refs/functional/#karbonn.functional.symmetric_absolute_relative_error","title":"karbonn.functional.symmetric_absolute_relative_error","text":"<pre><code>symmetric_absolute_relative_error(\n    prediction: Tensor, target: Tensor, eps: float = 1e-08\n) -&gt; Tensor\n</code></pre> <p>Compute the element-wise symmetric absolute relative error between the predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The tensor of predictions.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor, which must have the same shape and data type as <code>prediction</code>.</p> required <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The symmetric absolute relative error tensor, which has the same shape and data type as the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.functional import symmetric_absolute_relative_error\n&gt;&gt;&gt; symmetric_absolute_relative_error(torch.eye(2), torch.ones(2, 2))\ntensor([[0., 2.],\n        [2., 0.]])\n</code></pre>"},{"location":"refs/functional/#utility","title":"Utility","text":""},{"location":"refs/functional/#karbonn.functional.check_loss_reduction_strategy","title":"karbonn.functional.check_loss_reduction_strategy","text":"<pre><code>check_loss_reduction_strategy(reduction: str) -&gt; None\n</code></pre> <p>Check if the provided reduction ia a valid loss reduction.</p> <p>The valid reduction values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The reduction strategy to check.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the provided reduction is not valid.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.functional import check_loss_reduction_strategy\n&gt;&gt;&gt; check_loss_reduction_strategy(\"mean\")\n</code></pre>"},{"location":"refs/metric/","title":"karbonn.metric","text":""},{"location":"refs/metric/#karbonn.metric","title":"karbonn.metric","text":"<p>Contain the metrics.</p>"},{"location":"refs/metric/#karbonn.metric.AbsoluteError","title":"karbonn.metric.AbsoluteError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the absolute error metric.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import AbsoluteError\n&gt;&gt;&gt; metric = AbsoluteError()\n&gt;&gt;&gt; metric\nAbsoluteError(\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.16666666666666666,\n 'min': 0.0,\n 'max': 1.0,\n 'sum': 2.0,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(prefix=\"abs_err_\")\n{'abs_err_mean': 0.5,\n 'abs_err_min': 0.0,\n 'abs_err_max': 1.0,\n 'abs_err_sum': 2.0,\n 'abs_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.AbsoluteError.forward","title":"karbonn.metric.AbsoluteError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the mean absolute error metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import AbsoluteError\n&gt;&gt;&gt; metric = AbsoluteError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.AbsoluteRelativeError","title":"karbonn.metric.AbsoluteRelativeError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the absolute relative error metric.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import AbsoluteRelativeError\n&gt;&gt;&gt; metric = AbsoluteRelativeError()\n&gt;&gt;&gt; metric\nAbsoluteRelativeError(\n  (eps): 1e-08\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.16666666666666666,\n 'min': 0.0,\n 'max': 1.0,\n 'sum': 2.0,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(prefix=\"abs_rel_err_\")\n{'abs_rel_err_mean': 0.5,\n 'abs_rel_err_min': 0.0,\n 'abs_rel_err_max': 1.0,\n 'abs_rel_err_sum': 2.0,\n 'abs_rel_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.AbsoluteRelativeError.forward","title":"karbonn.metric.AbsoluteRelativeError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the mean absolute percentage error metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import AbsoluteRelativeError\n&gt;&gt;&gt; metric = AbsoluteRelativeError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.Accuracy","title":"karbonn.metric.Accuracy","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement a categorical accuracy metric.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>AccuracyState</code> is instantiated.</p> <code>None</code> <code>transform</code> <code>Module | dict | None</code> <p>The transformation applied on the predictions to generate the predicted categorical labels. If <code>None</code>, the identity module is used. The transform module must take a single input tensor and output a single tensor.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import Accuracy\n&gt;&gt;&gt; metric = Accuracy()\n&gt;&gt;&gt; metric\nAccuracy(\n  (state): AccuracyState(\n      (tracker): MeanTensorTracker(count=0, total=0.0)\n      (track_count): True\n    )\n  (transform): Identity()\n)\n&gt;&gt;&gt; metric(torch.tensor([1, 0]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0, 'count': 2}\n&gt;&gt;&gt; metric(torch.tensor([[0, 2]]), torch.tensor([1, 2]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.75, 'count': 4}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.tensor([[1, 1]]), torch.tensor([1, 2]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.5, 'count': 2}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.Accuracy.forward","title":"karbonn.metric.Accuracy.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the accuracy metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels or the predictions. This input must be a <code>torch.Tensor</code> of  shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, num_classes)</code> and type long or float. If the input is the predictions/scores, then the <code>transform</code> module should be set to transform the predictions/scores to categorical labels where the values are in <code>{0, 1, ..., num_classes-1}</code>.</p> required <code>target</code> <code>Tensor</code> <p>The categorical targets. The values have to be in <code>{0, 1, ..., num_classes-1}</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import Accuracy\n&gt;&gt;&gt; metric = Accuracy()\n&gt;&gt;&gt; metric(torch.tensor([1, 2, 0, 1]), torch.tensor([1, 2, 0, 1]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0, 'count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.BaseMetric","title":"karbonn.metric.BaseMetric","text":"<p>               Bases: <code>Module</code></p> <p>Define the base class to implement a metric.</p> <p>This class is used to register the metric using the metaclass factory. Child classes must implement the following methods:</p> <pre><code>- ``forward``\n- ``get_records``\n- ``reset``\n- ``value``\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.BaseMetric.get_records","title":"karbonn.metric.BaseMetric.get_records  <code>abstractmethod</code>","text":"<pre><code>get_records(\n    prefix: str = \"\", suffix: str = \"\"\n) -&gt; tuple[BaseRecord, ...]\n</code></pre> <p>Get the records for the metrics associated to the current state.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The key prefix in the record names.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the record names.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[BaseRecord, ...]</code> <p>The records.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.metric import AbsoluteError\n&gt;&gt;&gt; metric = AbsoluteError()\n&gt;&gt;&gt; metric.get_records(\"error_\")\n(MinScalarRecord(name=error_mean, max_size=10, size=0),\n MinScalarRecord(name=error_min, max_size=10, size=0),\n MinScalarRecord(name=error_max, max_size=10, size=0),\n MinScalarRecord(name=error_sum, max_size=10, size=0))\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.BaseMetric.reset","title":"karbonn.metric.BaseMetric.reset  <code>abstractmethod</code>","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the metric.</p>"},{"location":"refs/metric/#karbonn.metric.BaseMetric.value","title":"karbonn.metric.BaseMetric.value  <code>abstractmethod</code>","text":"<pre><code>value(prefix: str = '', suffix: str = '') -&gt; dict\n</code></pre> <p>Evaluate the metric and return the results given all the examples previously seen.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict</code> <p>The results of the metric.</p>"},{"location":"refs/metric/#karbonn.metric.BaseStateMetric","title":"karbonn.metric.BaseStateMetric","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Define a base class to implement a metric with a state.</p> <p>Child classes must implement the following method:</p> <pre><code>- ``forward``\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict</code> <p>The metric state or its configuration.</p> required"},{"location":"refs/metric/#karbonn.metric.BinaryConfusionMatrix","title":"karbonn.metric.BinaryConfusionMatrix","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Implement a confusion tracker metric for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>betas</code> <code>Sequence[int | float]</code> <p>The betas used to compute the f-beta scores.</p> <code>(1)</code> <code>tracker</code> <code>BinaryConfusionMatrixTracker | None</code> <p>The value tracker. If <code>None</code>, a <code>BinaryConfusionMatrixTracker</code> object is initialized.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import BinaryConfusionMatrix\n&gt;&gt;&gt; metric = BinaryConfusionMatrix()\n&gt;&gt;&gt; metric\nBinaryConfusionMatrix(\n  (betas): (1,)\n  (tracker): BinaryConfusionMatrixTracker(num_classes=2, count=0)\n  (track_count): True\n)\n&gt;&gt;&gt; metric(torch.tensor([0, 1, 0, 1]), torch.tensor([0, 1, 0, 1]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'jaccard_index': 1.0,\n 'count': 4,\n 'precision': 1.0,\n 'recall': 1.0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 2,\n 'f1_score': 1.0}\n&gt;&gt;&gt; metric(torch.tensor([1, 0]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'jaccard_index': 1.0,\n 'count': 6,\n 'precision': 1.0,\n 'recall': 1.0,\n 'true_negative_rate': 1.0,\n 'true_negative': 3,\n 'true_positive_rate': 1.0,\n 'true_positive': 3,\n 'f1_score': 1.0}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.tensor([1, 0]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'jaccard_index': 1.0,\n 'count': 2,\n 'precision': 1.0,\n 'recall': 1.0,\n 'true_negative_rate': 1.0,\n 'true_negative': 1,\n 'true_positive_rate': 1.0,\n 'true_positive': 1,\n 'f1_score': 1.0}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.BinaryConfusionMatrix.forward","title":"karbonn.metric.BinaryConfusionMatrix.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the confusion tracker metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels where the values are <code>0</code> or <code>1</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <code>target</code> <code>Tensor</code> <p>The binary targets where the values are <code>0</code> or <code>1</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or  <code>(d0, d1, ..., dn, 1)</code> and type bool or long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import BinaryConfusionMatrix\n&gt;&gt;&gt; metric = BinaryConfusionMatrix()\n&gt;&gt;&gt; metric(torch.tensor([0, 1, 0, 1]), torch.tensor([0, 1, 0, 1]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'jaccard_index': 1.0,\n 'count': 4,\n 'precision': 1.0,\n 'recall': 1.0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 2,\n 'f1_score': 1.0}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.CategoricalConfusionMatrix","title":"karbonn.metric.CategoricalConfusionMatrix","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Implement a confusion tracker metric for multi-class labels.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes.</p> required <code>betas</code> <code>Sequence[int | float]</code> <p>The betas used to compute the f-beta scores.</p> <code>(1)</code> <code>tracker</code> <code>MulticlassConfusionMatrixTracker | None</code> <p>The value tracker. If <code>None</code>, a <code>BinaryConfusionMatrixTracker</code> object is initialized.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import CategoricalConfusionMatrix\n&gt;&gt;&gt; metric = CategoricalConfusionMatrix(num_classes=3)\n&gt;&gt;&gt; metric\nCategoricalConfusionMatrix(\n  (betas): (1,)\n  (tracker): MulticlassConfusionMatrixTracker(num_classes=3, count=0)\n  (track_count): True\n)\n&gt;&gt;&gt; metric(\n...     prediction=torch.tensor([0, 1, 2, 0, 0, 1]),\n...     target=torch.tensor([2, 2, 2, 0, 0, 0]),\n... )\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.5,\n 'balanced_accuracy': 0.333333...,\n 'count': 6,\n 'macro_precision': 0.555555...,\n 'macro_recall': 0.333333...,\n 'macro_f1_score': 0.388888...,\n 'micro_precision': 0.5,\n 'micro_recall': 0.5,\n 'micro_f1_score': 0.5,\n 'weighted_precision': 0.833333...,\n 'weighted_recall': 0.5,\n 'weighted_f1_score': 0.583333...,\n 'precision': tensor([0.6667, 0.0000, 1.0000]),\n 'recall': tensor([0.6667, 0.0000, 0.3333]),\n 'f1_score': tensor([0.6667, 0.0000, 0.5000])}\n&gt;&gt;&gt; metric(prediction=torch.tensor([1, 0]), target=torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.625,\n 'balanced_accuracy': 0.694444...,\n 'count': 8,\n 'macro_precision': 0.694444...,\n 'macro_recall': 0.694444...,\n 'macro_f1_score': 0.583333...,\n 'micro_precision': 0.625,\n 'micro_recall': 0.625,\n 'micro_f1_score': 0.625,\n 'weighted_precision': 0.791666...,\n 'weighted_recall': 0.625,\n 'weighted_f1_score': 0.625,\n 'precision': tensor([0.7500, 0.3333, 1.0000]),\n 'recall': tensor([0.7500, 1.0000, 0.3333]),\n 'f1_score': tensor([0.7500, 0.5000, 0.5000])}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(prediction=torch.tensor([1, 0, 2]), target=torch.tensor([1, 0, 2]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'count': 3,\n 'macro_precision': 1.0,\n 'macro_recall': 1.0,\n 'macro_f1_score': 1.0,\n 'micro_precision': 1.0,\n 'micro_recall': 1.0,\n 'micro_f1_score': 1.0,\n 'weighted_precision': 1.0,\n 'weighted_recall': 1.0,\n 'weighted_f1_score': 1.0,\n 'precision': tensor([1., 1., 1.]),\n 'recall': tensor([1., 1., 1.]),\n 'f1_score': tensor([1., 1., 1.])}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.CategoricalConfusionMatrix.forward","title":"karbonn.metric.CategoricalConfusionMatrix.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the confusion tracker metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels where the values are <code>0</code> or <code>1</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <code>target</code> <code>Tensor</code> <p>The binary targets where the values are <code>0</code> or <code>1</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or  <code>(d0, d1, ..., dn, 1)</code> and type bool or long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import CategoricalConfusionMatrix\n&gt;&gt;&gt; metric = CategoricalConfusionMatrix(num_classes=3)\n&gt;&gt;&gt; metric(\n...     prediction=torch.tensor([0, 1, 2, 0, 0, 1]),\n...     target=torch.tensor([2, 2, 2, 0, 0, 0]),\n... )\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.5,\n 'balanced_accuracy': 0.333333...,\n 'count': 6,\n 'macro_precision': 0.555555...,\n 'macro_recall': 0.333333...,\n 'macro_f1_score': 0.388888...,\n 'micro_precision': 0.5,\n 'micro_recall': 0.5,\n 'micro_f1_score': 0.5,\n 'weighted_precision': 0.833333...,\n 'weighted_recall': 0.5,\n 'weighted_f1_score': 0.583333...,\n 'precision': tensor([0.6667, 0.0000, 1.0000]),\n 'recall': tensor([0.6667, 0.0000, 0.3333]),\n 'f1_score': tensor([0.6667, 0.0000, 0.5000])}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.CategoricalCrossEntropy","title":"karbonn.metric.CategoricalCrossEntropy","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement a metric to compute the categorical cross-entropy.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>MeanErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import CategoricalCrossEntropy\n&gt;&gt;&gt; metric = CategoricalCrossEntropy()\n&gt;&gt;&gt; metric\nCategoricalCrossEntropy(\n  (state): MeanErrorState(\n      (tracker): MeanTensorTracker(count=0, total=0.0)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.eye(4), torch.arange(4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.743668..., 'count': 4}\n&gt;&gt;&gt; metric(torch.ones(2, 3), torch.ones(2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.861983..., 'count': 6}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.ones(2, 3), torch.ones(2))\n&gt;&gt;&gt; metric.value()\n{'mean': 1.098612..., 'count': 2}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.CategoricalCrossEntropy.forward","title":"karbonn.metric.CategoricalCrossEntropy.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, num_classes)</code> and type float.</p> required <code>target</code> <code>Tensor</code> <p>The categorical targets. The values have to be in <code>{0, 1, ..., num_classes-1}</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import CategoricalCrossEntropy\n&gt;&gt;&gt; metric = CategoricalCrossEntropy()\n&gt;&gt;&gt; metric(torch.eye(4), torch.arange(4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.743668..., 'count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.EmptyMetricError","title":"karbonn.metric.EmptyMetricError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an empty metric is evaluated.</p>"},{"location":"refs/metric/#karbonn.metric.LogCoshError","title":"karbonn.metric.LogCoshError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement a metric to compute the logarithm of the hyperbolic cosine of the prediction error.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>The scale factor.</p> <code>1.0</code> <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import LogCoshError\n&gt;&gt;&gt; metric = LogCoshError()\n&gt;&gt;&gt; metric\nLogCoshError(\n  (scale): 1.0\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.072296...,\n 'min': 0.0,\n 'max': 0.433780...,\n 'sum': 0.867561...,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"log_cosh_err_\")\n{'log_cosh_err_mean': 0.216890...,\n 'log_cosh_err_min': 0.0,\n 'log_cosh_err_max': 0.433780...,\n 'log_cosh_err_sum': 0.867561...,\n 'log_cosh_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.LogCoshError.forward","title":"karbonn.metric.LogCoshError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import LogCoshError\n&gt;&gt;&gt; metric = LogCoshError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.NormalizedMeanSquaredError","title":"karbonn.metric.NormalizedMeanSquaredError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the normalized mean squared error (NMSE) metric.</p> <p>Note: this metric does not work if all the targets are zero.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>NormalizedMeanSquaredErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import NormalizedMeanSquaredError\n&gt;&gt;&gt; metric = NormalizedMeanSquaredError()\n&gt;&gt;&gt; metric\nNormalizedMeanSquaredError(\n  (state): NormalizedMeanSquaredErrorState(\n      (squared_errors): MeanTensorTracker(count=0, total=0.0)\n      (squared_targets): MeanTensorTracker(count=0, total=0.0)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0, 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.166666..., 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.5, 'count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.NormalizedMeanSquaredError.forward","title":"karbonn.metric.NormalizedMeanSquaredError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the normalized mean squared error metric given a mini- batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import NormalizedMeanSquaredError\n&gt;&gt;&gt; metric = NormalizedMeanSquaredError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0, 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.RootMeanSquaredError","title":"karbonn.metric.RootMeanSquaredError","text":"<p>               Bases: <code>SquaredError</code></p> <p>Implement the squared error metric.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>RootMeanErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import RootMeanSquaredError\n&gt;&gt;&gt; metric = RootMeanSquaredError()\n&gt;&gt;&gt; metric\nRootMeanSquaredError(\n  (state): RootMeanErrorState(\n      (tracker): MeanTensorTracker(count=0, total=0.0)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0, 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.408248..., 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.707106..., 'count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.SquaredAsinhError","title":"karbonn.metric.SquaredAsinhError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement a metric to compute the squared error on the inverse hyperbolic sine (arcsinh) transformed predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredAsinhError\n&gt;&gt;&gt; metric = SquaredAsinhError()\n&gt;&gt;&gt; metric\nSquaredAsinhError(\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.129469...,\n 'min': 0.0,\n 'max': 0.776819...,\n 'sum': 1.553638...,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"sq_asinh_err_\")\n{'sq_asinh_err_mean': 0.388409...,\n 'sq_asinh_err_min': 0.0,\n 'sq_asinh_err_max': 0.776819...,\n 'sq_asinh_err_sum': 1.553638...,\n 'sq_asinh_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.SquaredAsinhError.forward","title":"karbonn.metric.SquaredAsinhError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the squared error on the inverse hyperbolic sine (arcsinh) transformed predictions and targets given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredAsinhError\n&gt;&gt;&gt; metric = SquaredAsinhError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.SquaredError","title":"karbonn.metric.SquaredError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the squared error metric.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredError\n&gt;&gt;&gt; metric = SquaredError()\n&gt;&gt;&gt; metric\nSquaredError(\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.166666...,\n 'min': 0.0,\n 'max': 1.0,\n 'sum': 2.0,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"sq_err_\")\n{'sq_err_mean': 0.5,\n 'sq_err_min': 0.0,\n 'sq_err_max': 1.0,\n 'sq_err_sum': 2.0,\n 'sq_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.SquaredError.forward","title":"karbonn.metric.SquaredError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the squared logarithmic error metric given a mini- batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredError\n&gt;&gt;&gt; metric = SquaredError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.SquaredLogError","title":"karbonn.metric.SquaredLogError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the squared logarithmic error (SLE) metric.</p> <p>This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.</p> <p>Note: this metric only works with positive value (0 included).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredLogError\n&gt;&gt;&gt; metric = SquaredLogError()\n&gt;&gt;&gt; metric\nSquaredLogError(\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.080075...,\n 'min': 0.0,\n 'max': 0.480453...,\n 'sum': 0.960906...,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"sq_log_err_\")\n{'sq_log_err_mean': 0.240226...,\n 'sq_log_err_min': 0.0,\n 'sq_log_err_max': 0.480453...,\n 'sq_log_err_sum': 0.960906...,\n 'sq_log_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.SquaredLogError.forward","title":"karbonn.metric.SquaredLogError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the squared logarithmic error metric given a mini- batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredLogError\n&gt;&gt;&gt; metric = SquaredLogError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.SymmetricAbsoluteRelativeError","title":"karbonn.metric.SymmetricAbsoluteRelativeError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the symmetric absolute relative error (SARE) metric.</p> <p>This metric tracks the mean, maximum and minimum absolute relative error values.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SymmetricAbsoluteRelativeError\n&gt;&gt;&gt; metric = SymmetricAbsoluteRelativeError()\n&gt;&gt;&gt; metric\nSymmetricAbsoluteRelativeError(\n  (eps): 1e-08\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.3333333333333333,\n 'min': 0.0,\n 'max': 2.0,\n 'sum': 4.0,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"sym_abs_rel_err_\")\n{'sym_abs_rel_err_mean': 1.0,\n 'sym_abs_rel_err_min': 0.0,\n 'sym_abs_rel_err_max': 2.0,\n 'sym_abs_rel_err_sum': 4.0,\n 'sym_abs_rel_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.SymmetricAbsoluteRelativeError.forward","title":"karbonn.metric.SymmetricAbsoluteRelativeError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the mean absolute percentage error metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SymmetricAbsoluteRelativeError\n&gt;&gt;&gt; metric = SymmetricAbsoluteRelativeError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.TopKAccuracy","title":"karbonn.metric.TopKAccuracy","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Implement the accuracy at k metric a.k.a. top-k accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>topk</code> <code>Sequence[int]</code> <p>The k values used to evaluate the top-k accuracy metric.</p> <code>(1, 5)</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import TopKAccuracy\n&gt;&gt;&gt; metric = TopKAccuracy(topk=(1,))\n&gt;&gt;&gt; metric\nTopKAccuracy(\n  (topk): (1,)\n  (states):\n    (1): AccuracyState(\n          (tracker): MeanTensorTracker(count=0, total=0.0)\n          (track_count): True\n        )\n)\n&gt;&gt;&gt; metric(torch.tensor([[0.0, 2.0, 1.0], [2.0, 1.0, 0.0]]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'top_1_accuracy': 1.0, 'top_1_count': 2}\n&gt;&gt;&gt; metric(torch.tensor([[0.0, 2.0, 1.0], [2.0, 1.0, 0.0]]), torch.tensor([1, 2]))\n&gt;&gt;&gt; metric.value()\n{'top_1_accuracy': 0.75, 'top_1_count': 4}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.tensor([[0.0, 2.0, 1.0], [2.0, 1.0, 0.0]]), torch.tensor([1, 2]))\n&gt;&gt;&gt; metric.value(\"acc_\")\n{'acc_top_1_accuracy': 0.5, 'acc_top_1_count': 2}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.TopKAccuracy.forward","title":"karbonn.metric.TopKAccuracy.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the accuracy metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, num_classes)</code> and type long or float.</p> required <code>target</code> <code>Tensor</code> <p>The categorical targets. The values have to be in <code>{0, 1, ..., num_classes-1}</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import TopKAccuracy\n&gt;&gt;&gt; metric = TopKAccuracy(topk=(1,))\n&gt;&gt;&gt; metric(torch.tensor([[0.0, 2.0, 1.0], [2.0, 1.0, 0.0]]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'top_1_accuracy': 1.0, 'top_1_count': 2}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.setup_metric","title":"karbonn.metric.setup_metric","text":"<pre><code>setup_metric(metric: BaseMetric | dict) -&gt; BaseMetric\n</code></pre> <p>Set up the metric.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>BaseMetric | dict</code> <p>The metric or its configuration.</p> required <p>Returns:</p> Type Description <code>BaseMetric</code> <p>The instantiated metric.</p>"},{"location":"refs/metric/#karbonn.metric.classification","title":"karbonn.metric.classification","text":"<p>Contain classification metrics.</p>"},{"location":"refs/metric/#karbonn.metric.classification.Accuracy","title":"karbonn.metric.classification.Accuracy","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement a categorical accuracy metric.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>AccuracyState</code> is instantiated.</p> <code>None</code> <code>transform</code> <code>Module | dict | None</code> <p>The transformation applied on the predictions to generate the predicted categorical labels. If <code>None</code>, the identity module is used. The transform module must take a single input tensor and output a single tensor.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import Accuracy\n&gt;&gt;&gt; metric = Accuracy()\n&gt;&gt;&gt; metric\nAccuracy(\n  (state): AccuracyState(\n      (tracker): MeanTensorTracker(count=0, total=0.0)\n      (track_count): True\n    )\n  (transform): Identity()\n)\n&gt;&gt;&gt; metric(torch.tensor([1, 0]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0, 'count': 2}\n&gt;&gt;&gt; metric(torch.tensor([[0, 2]]), torch.tensor([1, 2]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.75, 'count': 4}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.tensor([[1, 1]]), torch.tensor([1, 2]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.5, 'count': 2}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.Accuracy.forward","title":"karbonn.metric.classification.Accuracy.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the accuracy metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels or the predictions. This input must be a <code>torch.Tensor</code> of  shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, num_classes)</code> and type long or float. If the input is the predictions/scores, then the <code>transform</code> module should be set to transform the predictions/scores to categorical labels where the values are in <code>{0, 1, ..., num_classes-1}</code>.</p> required <code>target</code> <code>Tensor</code> <p>The categorical targets. The values have to be in <code>{0, 1, ..., num_classes-1}</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import Accuracy\n&gt;&gt;&gt; metric = Accuracy()\n&gt;&gt;&gt; metric(torch.tensor([1, 2, 0, 1]), torch.tensor([1, 2, 0, 1]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0, 'count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.BinaryConfusionMatrix","title":"karbonn.metric.classification.BinaryConfusionMatrix","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Implement a confusion tracker metric for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>betas</code> <code>Sequence[int | float]</code> <p>The betas used to compute the f-beta scores.</p> <code>(1)</code> <code>tracker</code> <code>BinaryConfusionMatrixTracker | None</code> <p>The value tracker. If <code>None</code>, a <code>BinaryConfusionMatrixTracker</code> object is initialized.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import BinaryConfusionMatrix\n&gt;&gt;&gt; metric = BinaryConfusionMatrix()\n&gt;&gt;&gt; metric\nBinaryConfusionMatrix(\n  (betas): (1,)\n  (tracker): BinaryConfusionMatrixTracker(num_classes=2, count=0)\n  (track_count): True\n)\n&gt;&gt;&gt; metric(torch.tensor([0, 1, 0, 1]), torch.tensor([0, 1, 0, 1]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'jaccard_index': 1.0,\n 'count': 4,\n 'precision': 1.0,\n 'recall': 1.0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 2,\n 'f1_score': 1.0}\n&gt;&gt;&gt; metric(torch.tensor([1, 0]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'jaccard_index': 1.0,\n 'count': 6,\n 'precision': 1.0,\n 'recall': 1.0,\n 'true_negative_rate': 1.0,\n 'true_negative': 3,\n 'true_positive_rate': 1.0,\n 'true_positive': 3,\n 'f1_score': 1.0}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.tensor([1, 0]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'jaccard_index': 1.0,\n 'count': 2,\n 'precision': 1.0,\n 'recall': 1.0,\n 'true_negative_rate': 1.0,\n 'true_negative': 1,\n 'true_positive_rate': 1.0,\n 'true_positive': 1,\n 'f1_score': 1.0}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.BinaryConfusionMatrix.forward","title":"karbonn.metric.classification.BinaryConfusionMatrix.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the confusion tracker metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels where the values are <code>0</code> or <code>1</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <code>target</code> <code>Tensor</code> <p>The binary targets where the values are <code>0</code> or <code>1</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or  <code>(d0, d1, ..., dn, 1)</code> and type bool or long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import BinaryConfusionMatrix\n&gt;&gt;&gt; metric = BinaryConfusionMatrix()\n&gt;&gt;&gt; metric(torch.tensor([0, 1, 0, 1]), torch.tensor([0, 1, 0, 1]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'jaccard_index': 1.0,\n 'count': 4,\n 'precision': 1.0,\n 'recall': 1.0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 2,\n 'f1_score': 1.0}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.CategoricalConfusionMatrix","title":"karbonn.metric.classification.CategoricalConfusionMatrix","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Implement a confusion tracker metric for multi-class labels.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>The number of classes.</p> required <code>betas</code> <code>Sequence[int | float]</code> <p>The betas used to compute the f-beta scores.</p> <code>(1)</code> <code>tracker</code> <code>MulticlassConfusionMatrixTracker | None</code> <p>The value tracker. If <code>None</code>, a <code>BinaryConfusionMatrixTracker</code> object is initialized.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import CategoricalConfusionMatrix\n&gt;&gt;&gt; metric = CategoricalConfusionMatrix(num_classes=3)\n&gt;&gt;&gt; metric\nCategoricalConfusionMatrix(\n  (betas): (1,)\n  (tracker): MulticlassConfusionMatrixTracker(num_classes=3, count=0)\n  (track_count): True\n)\n&gt;&gt;&gt; metric(\n...     prediction=torch.tensor([0, 1, 2, 0, 0, 1]),\n...     target=torch.tensor([2, 2, 2, 0, 0, 0]),\n... )\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.5,\n 'balanced_accuracy': 0.333333...,\n 'count': 6,\n 'macro_precision': 0.555555...,\n 'macro_recall': 0.333333...,\n 'macro_f1_score': 0.388888...,\n 'micro_precision': 0.5,\n 'micro_recall': 0.5,\n 'micro_f1_score': 0.5,\n 'weighted_precision': 0.833333...,\n 'weighted_recall': 0.5,\n 'weighted_f1_score': 0.583333...,\n 'precision': tensor([0.6667, 0.0000, 1.0000]),\n 'recall': tensor([0.6667, 0.0000, 0.3333]),\n 'f1_score': tensor([0.6667, 0.0000, 0.5000])}\n&gt;&gt;&gt; metric(prediction=torch.tensor([1, 0]), target=torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.625,\n 'balanced_accuracy': 0.694444...,\n 'count': 8,\n 'macro_precision': 0.694444...,\n 'macro_recall': 0.694444...,\n 'macro_f1_score': 0.583333...,\n 'micro_precision': 0.625,\n 'micro_recall': 0.625,\n 'micro_f1_score': 0.625,\n 'weighted_precision': 0.791666...,\n 'weighted_recall': 0.625,\n 'weighted_f1_score': 0.625,\n 'precision': tensor([0.7500, 0.3333, 1.0000]),\n 'recall': tensor([0.7500, 1.0000, 0.3333]),\n 'f1_score': tensor([0.7500, 0.5000, 0.5000])}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(prediction=torch.tensor([1, 0, 2]), target=torch.tensor([1, 0, 2]))\n&gt;&gt;&gt; metric.value()\n{'accuracy': 1.0,\n 'balanced_accuracy': 1.0,\n 'count': 3,\n 'macro_precision': 1.0,\n 'macro_recall': 1.0,\n 'macro_f1_score': 1.0,\n 'micro_precision': 1.0,\n 'micro_recall': 1.0,\n 'micro_f1_score': 1.0,\n 'weighted_precision': 1.0,\n 'weighted_recall': 1.0,\n 'weighted_f1_score': 1.0,\n 'precision': tensor([1., 1., 1.]),\n 'recall': tensor([1., 1., 1.]),\n 'f1_score': tensor([1., 1., 1.])}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.CategoricalConfusionMatrix.forward","title":"karbonn.metric.classification.CategoricalConfusionMatrix.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the confusion tracker metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels where the values are <code>0</code> or <code>1</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <code>target</code> <code>Tensor</code> <p>The binary targets where the values are <code>0</code> or <code>1</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or  <code>(d0, d1, ..., dn, 1)</code> and type bool or long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import CategoricalConfusionMatrix\n&gt;&gt;&gt; metric = CategoricalConfusionMatrix(num_classes=3)\n&gt;&gt;&gt; metric(\n...     prediction=torch.tensor([0, 1, 2, 0, 0, 1]),\n...     target=torch.tensor([2, 2, 2, 0, 0, 0]),\n... )\n&gt;&gt;&gt; metric.value()\n{'accuracy': 0.5,\n 'balanced_accuracy': 0.333333...,\n 'count': 6,\n 'macro_precision': 0.555555...,\n 'macro_recall': 0.333333...,\n 'macro_f1_score': 0.388888...,\n 'micro_precision': 0.5,\n 'micro_recall': 0.5,\n 'micro_f1_score': 0.5,\n 'weighted_precision': 0.833333...,\n 'weighted_recall': 0.5,\n 'weighted_f1_score': 0.583333...,\n 'precision': tensor([0.6667, 0.0000, 1.0000]),\n 'recall': tensor([0.6667, 0.0000, 0.3333]),\n 'f1_score': tensor([0.6667, 0.0000, 0.5000])}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.CategoricalCrossEntropy","title":"karbonn.metric.classification.CategoricalCrossEntropy","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement a metric to compute the categorical cross-entropy.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>MeanErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import CategoricalCrossEntropy\n&gt;&gt;&gt; metric = CategoricalCrossEntropy()\n&gt;&gt;&gt; metric\nCategoricalCrossEntropy(\n  (state): MeanErrorState(\n      (tracker): MeanTensorTracker(count=0, total=0.0)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.eye(4), torch.arange(4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.743668..., 'count': 4}\n&gt;&gt;&gt; metric(torch.ones(2, 3), torch.ones(2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.861983..., 'count': 6}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.ones(2, 3), torch.ones(2))\n&gt;&gt;&gt; metric.value()\n{'mean': 1.098612..., 'count': 2}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.CategoricalCrossEntropy.forward","title":"karbonn.metric.classification.CategoricalCrossEntropy.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, num_classes)</code> and type float.</p> required <code>target</code> <code>Tensor</code> <p>The categorical targets. The values have to be in <code>{0, 1, ..., num_classes-1}</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import CategoricalCrossEntropy\n&gt;&gt;&gt; metric = CategoricalCrossEntropy()\n&gt;&gt;&gt; metric(torch.eye(4), torch.arange(4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.743668..., 'count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.TopKAccuracy","title":"karbonn.metric.classification.TopKAccuracy","text":"<p>               Bases: <code>BaseMetric</code></p> <p>Implement the accuracy at k metric a.k.a. top-k accuracy.</p> <p>Parameters:</p> Name Type Description Default <code>topk</code> <code>Sequence[int]</code> <p>The k values used to evaluate the top-k accuracy metric.</p> <code>(1, 5)</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import TopKAccuracy\n&gt;&gt;&gt; metric = TopKAccuracy(topk=(1,))\n&gt;&gt;&gt; metric\nTopKAccuracy(\n  (topk): (1,)\n  (states):\n    (1): AccuracyState(\n          (tracker): MeanTensorTracker(count=0, total=0.0)\n          (track_count): True\n        )\n)\n&gt;&gt;&gt; metric(torch.tensor([[0.0, 2.0, 1.0], [2.0, 1.0, 0.0]]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'top_1_accuracy': 1.0, 'top_1_count': 2}\n&gt;&gt;&gt; metric(torch.tensor([[0.0, 2.0, 1.0], [2.0, 1.0, 0.0]]), torch.tensor([1, 2]))\n&gt;&gt;&gt; metric.value()\n{'top_1_accuracy': 0.75, 'top_1_count': 4}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.tensor([[0.0, 2.0, 1.0], [2.0, 1.0, 0.0]]), torch.tensor([1, 2]))\n&gt;&gt;&gt; metric.value(\"acc_\")\n{'acc_top_1_accuracy': 0.5, 'acc_top_1_count': 2}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.classification.TopKAccuracy.forward","title":"karbonn.metric.classification.TopKAccuracy.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the accuracy metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted labels as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, num_classes)</code> and type long or float.</p> required <code>target</code> <code>Tensor</code> <p>The categorical targets. The values have to be in <code>{0, 1, ..., num_classes-1}</code>. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> or <code>(d0, d1, ..., dn, 1)</code> and type long or float.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import TopKAccuracy\n&gt;&gt;&gt; metric = TopKAccuracy(topk=(1,))\n&gt;&gt;&gt; metric(torch.tensor([[0.0, 2.0, 1.0], [2.0, 1.0, 0.0]]), torch.tensor([1, 0]))\n&gt;&gt;&gt; metric.value()\n{'top_1_accuracy': 1.0, 'top_1_count': 2}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression","title":"karbonn.metric.regression","text":"<p>Contain the regression metrics.</p>"},{"location":"refs/metric/#karbonn.metric.regression.AbsoluteError","title":"karbonn.metric.regression.AbsoluteError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the absolute error metric.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import AbsoluteError\n&gt;&gt;&gt; metric = AbsoluteError()\n&gt;&gt;&gt; metric\nAbsoluteError(\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.16666666666666666,\n 'min': 0.0,\n 'max': 1.0,\n 'sum': 2.0,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(prefix=\"abs_err_\")\n{'abs_err_mean': 0.5,\n 'abs_err_min': 0.0,\n 'abs_err_max': 1.0,\n 'abs_err_sum': 2.0,\n 'abs_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.AbsoluteError.forward","title":"karbonn.metric.regression.AbsoluteError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the mean absolute error metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import AbsoluteError\n&gt;&gt;&gt; metric = AbsoluteError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.AbsoluteRelativeError","title":"karbonn.metric.regression.AbsoluteRelativeError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the absolute relative error metric.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import AbsoluteRelativeError\n&gt;&gt;&gt; metric = AbsoluteRelativeError()\n&gt;&gt;&gt; metric\nAbsoluteRelativeError(\n  (eps): 1e-08\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.16666666666666666,\n 'min': 0.0,\n 'max': 1.0,\n 'sum': 2.0,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(prefix=\"abs_rel_err_\")\n{'abs_rel_err_mean': 0.5,\n 'abs_rel_err_min': 0.0,\n 'abs_rel_err_max': 1.0,\n 'abs_rel_err_sum': 2.0,\n 'abs_rel_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.AbsoluteRelativeError.forward","title":"karbonn.metric.regression.AbsoluteRelativeError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the mean absolute percentage error metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import AbsoluteRelativeError\n&gt;&gt;&gt; metric = AbsoluteRelativeError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.LogCoshError","title":"karbonn.metric.regression.LogCoshError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement a metric to compute the logarithm of the hyperbolic cosine of the prediction error.</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>float</code> <p>The scale factor.</p> <code>1.0</code> <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import LogCoshError\n&gt;&gt;&gt; metric = LogCoshError()\n&gt;&gt;&gt; metric\nLogCoshError(\n  (scale): 1.0\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.072296...,\n 'min': 0.0,\n 'max': 0.433780...,\n 'sum': 0.867561...,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"log_cosh_err_\")\n{'log_cosh_err_mean': 0.216890...,\n 'log_cosh_err_min': 0.0,\n 'log_cosh_err_max': 0.433780...,\n 'log_cosh_err_sum': 0.867561...,\n 'log_cosh_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.LogCoshError.forward","title":"karbonn.metric.regression.LogCoshError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import LogCoshError\n&gt;&gt;&gt; metric = LogCoshError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.NormalizedMeanSquaredError","title":"karbonn.metric.regression.NormalizedMeanSquaredError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the normalized mean squared error (NMSE) metric.</p> <p>Note: this metric does not work if all the targets are zero.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>NormalizedMeanSquaredErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import NormalizedMeanSquaredError\n&gt;&gt;&gt; metric = NormalizedMeanSquaredError()\n&gt;&gt;&gt; metric\nNormalizedMeanSquaredError(\n  (state): NormalizedMeanSquaredErrorState(\n      (squared_errors): MeanTensorTracker(count=0, total=0.0)\n      (squared_targets): MeanTensorTracker(count=0, total=0.0)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0, 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.166666..., 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.5, 'count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.NormalizedMeanSquaredError.forward","title":"karbonn.metric.regression.NormalizedMeanSquaredError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the normalized mean squared error metric given a mini- batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import NormalizedMeanSquaredError\n&gt;&gt;&gt; metric = NormalizedMeanSquaredError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0, 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.RootMeanSquaredError","title":"karbonn.metric.regression.RootMeanSquaredError","text":"<p>               Bases: <code>SquaredError</code></p> <p>Implement the squared error metric.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>RootMeanErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import RootMeanSquaredError\n&gt;&gt;&gt; metric = RootMeanSquaredError()\n&gt;&gt;&gt; metric\nRootMeanSquaredError(\n  (state): RootMeanErrorState(\n      (tracker): MeanTensorTracker(count=0, total=0.0)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0, 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.408248..., 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.707106..., 'count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.SquaredAsinhError","title":"karbonn.metric.regression.SquaredAsinhError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement a metric to compute the squared error on the inverse hyperbolic sine (arcsinh) transformed predictions and targets.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredAsinhError\n&gt;&gt;&gt; metric = SquaredAsinhError()\n&gt;&gt;&gt; metric\nSquaredAsinhError(\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.129469...,\n 'min': 0.0,\n 'max': 0.776819...,\n 'sum': 1.553638...,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"sq_asinh_err_\")\n{'sq_asinh_err_mean': 0.388409...,\n 'sq_asinh_err_min': 0.0,\n 'sq_asinh_err_max': 0.776819...,\n 'sq_asinh_err_sum': 1.553638...,\n 'sq_asinh_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.SquaredAsinhError.forward","title":"karbonn.metric.regression.SquaredAsinhError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the squared error on the inverse hyperbolic sine (arcsinh) transformed predictions and targets given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredAsinhError\n&gt;&gt;&gt; metric = SquaredAsinhError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.SquaredError","title":"karbonn.metric.regression.SquaredError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the squared error metric.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredError\n&gt;&gt;&gt; metric = SquaredError()\n&gt;&gt;&gt; metric\nSquaredError(\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.166666...,\n 'min': 0.0,\n 'max': 1.0,\n 'sum': 2.0,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"sq_err_\")\n{'sq_err_mean': 0.5,\n 'sq_err_min': 0.0,\n 'sq_err_max': 1.0,\n 'sq_err_sum': 2.0,\n 'sq_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.SquaredError.forward","title":"karbonn.metric.regression.SquaredError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the squared logarithmic error metric given a mini- batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredError\n&gt;&gt;&gt; metric = SquaredError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.SquaredLogError","title":"karbonn.metric.regression.SquaredLogError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the squared logarithmic error (SLE) metric.</p> <p>This metric is best to use when targets having exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate.</p> <p>Note: this metric only works with positive value (0 included).</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredLogError\n&gt;&gt;&gt; metric = SquaredLogError()\n&gt;&gt;&gt; metric\nSquaredLogError(\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.080075...,\n 'min': 0.0,\n 'max': 0.480453...,\n 'sum': 0.960906...,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"sq_log_err_\")\n{'sq_log_err_mean': 0.240226...,\n 'sq_log_err_min': 0.0,\n 'sq_log_err_max': 0.480453...,\n 'sq_log_err_sum': 0.960906...,\n 'sq_log_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.SquaredLogError.forward","title":"karbonn.metric.regression.SquaredLogError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the squared logarithmic error metric given a mini- batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SquaredLogError\n&gt;&gt;&gt; metric = SquaredLogError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.SymmetricAbsoluteRelativeError","title":"karbonn.metric.regression.SymmetricAbsoluteRelativeError","text":"<p>               Bases: <code>BaseStateMetric</code></p> <p>Implement the symmetric absolute relative error (SARE) metric.</p> <p>This metric tracks the mean, maximum and minimum absolute relative error values.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the target is zero.</p> <code>1e-08</code> <code>state</code> <code>BaseState | dict | None</code> <p>The metric state or its configuration. If <code>None</code>, <code>ErrorState</code> is instantiated.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SymmetricAbsoluteRelativeError\n&gt;&gt;&gt; metric = SymmetricAbsoluteRelativeError()\n&gt;&gt;&gt; metric\nSymmetricAbsoluteRelativeError(\n  (eps): 1e-08\n  (state): ErrorState(\n      (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n      (track_count): True\n    )\n)\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.3333333333333333,\n 'min': 0.0,\n 'max': 2.0,\n 'sum': 4.0,\n 'count': 12}\n&gt;&gt;&gt; metric.reset()\n&gt;&gt;&gt; metric(torch.eye(2), torch.ones(2, 2))\n&gt;&gt;&gt; metric.value(\"sym_abs_rel_err_\")\n{'sym_abs_rel_err_mean': 1.0,\n 'sym_abs_rel_err_min': 0.0,\n 'sym_abs_rel_err_max': 2.0,\n 'sym_abs_rel_err_sum': 4.0,\n 'sym_abs_rel_err_count': 4}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.regression.SymmetricAbsoluteRelativeError.forward","title":"karbonn.metric.regression.SymmetricAbsoluteRelativeError.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the mean absolute percentage error metric given a mini-batch of examples.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predictions as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <code>target</code> <code>Tensor</code> <p>The target tensor as a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn)</code> and type float or long.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric import SymmetricAbsoluteRelativeError\n&gt;&gt;&gt; metric = SymmetricAbsoluteRelativeError()\n&gt;&gt;&gt; metric(torch.ones(2, 4), torch.ones(2, 4))\n&gt;&gt;&gt; metric.value()\n{'mean': 0.0,\n 'min': 0.0,\n 'max': 0.0,\n 'sum': 0.0,\n 'count': 8}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state","title":"karbonn.metric.state","text":"<p>Contain the metric states.</p>"},{"location":"refs/metric/#karbonn.metric.state.AccuracyState","title":"karbonn.metric.state.AccuracyState","text":"<p>               Bases: <code>BaseState</code></p> <p>Implement a metric state to compute the accuracy.</p> <p>This state has a constant space complexity.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>MeanTensorTracker | None</code> <p>The mean value tracker.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import AccuracyState\n&gt;&gt;&gt; state = AccuracyState()\n&gt;&gt;&gt; state\nAccuracyState(\n  (tracker): MeanTensorTracker(count=0, total=0.0)\n  (track_count): True\n)\n&gt;&gt;&gt; state.get_records()\n(MaxScalarRecord(name=accuracy, max_size=10, size=0),)\n&gt;&gt;&gt; state.update(torch.eye(4))\n&gt;&gt;&gt; state.value()\n{'accuracy': 0.25, 'count': 16}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.AccuracyState.update","title":"karbonn.metric.state.AccuracyState.update","text":"<pre><code>update(correct: Tensor) -&gt; None\n</code></pre> <p>Update the metric state with a new tensor of errors.</p> <p>Parameters:</p> Name Type Description Default <code>correct</code> <code>Tensor</code> <p>A tensor that indicates the correct predictions. <code>1</code> indicates a correct prediction and <code>0</code> indicates a bad prediction.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import AccuracyState\n&gt;&gt;&gt; state = AccuracyState()\n&gt;&gt;&gt; state.update(torch.eye(4))\n&gt;&gt;&gt; state.value()\n{'accuracy': 0.25, 'count': 16}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.BaseState","title":"karbonn.metric.state.BaseState","text":"<p>               Bases: <code>ABC</code></p> <p>Define a base class to implement a metric state.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state\nErrorState(\n  (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n  (track_count): True\n)\n&gt;&gt;&gt; state.get_records(\"error_\")\n(MinScalarRecord(name=error_mean, max_size=10, size=0),\n MinScalarRecord(name=error_min, max_size=10, size=0),\n MinScalarRecord(name=error_max, max_size=10, size=0),\n MinScalarRecord(name=error_sum, max_size=10, size=0))\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 2.5,\n 'error_min': 0.0,\n 'error_max': 5.0,\n 'error_sum': 15.0,\n 'error_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.BaseState.count","title":"karbonn.metric.state.BaseState.count  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>The number of predictions in the state.</p>"},{"location":"refs/metric/#karbonn.metric.state.BaseState.clone","title":"karbonn.metric.state.BaseState.clone","text":"<pre><code>clone() -&gt; BaseState\n</code></pre> <p>Create a copy of the current state.</p> <p>Returns:</p> Type Description <code>BaseState</code> <p>A copy of the current state.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state_cloned = state.clone()\n&gt;&gt;&gt; state.update(torch.ones(3))\n&gt;&gt;&gt; state\nErrorState(\n  (tracker): ScalableTensorTracker(count=9, total=18.0, min_value=0, max_value=5)\n  (track_count): True\n)\n&gt;&gt;&gt; state_cloned\nErrorState(\n  (tracker): ScalableTensorTracker(count=6, total=15.0, min_value=0.0, max_value=5.0)\n  (track_count): True\n)\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.BaseState.equal","title":"karbonn.metric.state.BaseState.equal  <code>abstractmethod</code>","text":"<pre><code>equal(other: Any) -&gt; bool\n</code></pre> <p>Indicate if two metric states are equal or not.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>The other metric state to compare.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the two metric states are equal, otherwise <code>False</code>.</p> <pre><code>Example usage:\n</code></pre> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; from karbonn.utils.tracker import ScalableTensorTracker\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state.equal(ErrorState())\nTrue\n&gt;&gt;&gt; state.equal(\n...     ErrorState(ScalableTensorTracker(count=4, total=10.0, min_value=0.0, max_value=5.0))\n... )\nFalse\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.BaseState.get_records","title":"karbonn.metric.state.BaseState.get_records  <code>abstractmethod</code>","text":"<pre><code>get_records(\n    prefix: str = \"\", suffix: str = \"\"\n) -&gt; tuple[BaseRecord, ...]\n</code></pre> <p>Get the records for the metrics associated to the current state.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The key prefix in the record names.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the record names.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[BaseRecord, ...]</code> <p>The records.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state.get_records(\"error_\")\n(MinScalarRecord(name=error_mean, max_size=10, size=0),\n MinScalarRecord(name=error_min, max_size=10, size=0),\n MinScalarRecord(name=error_max, max_size=10, size=0),\n MinScalarRecord(name=error_sum, max_size=10, size=0))\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.BaseState.reset","title":"karbonn.metric.state.BaseState.reset  <code>abstractmethod</code>","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Reset the state.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state\nErrorState(\n  (tracker): ScalableTensorTracker(count=6, total=15.0, min_value=0, max_value=5)\n  (track_count): True\n)\n&gt;&gt;&gt; state.reset()\n&gt;&gt;&gt; state\nErrorState(\n  (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n  (track_count): True\n)\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.BaseState.update","title":"karbonn.metric.state.BaseState.update  <code>abstractmethod</code>","text":"<pre><code>update(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Update the metric state.</p> <p>The exact signature for this method depends on each metric state implementation.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state\nErrorState(\n  (tracker): ScalableTensorTracker(count=6, total=15.0, min_value=0, max_value=5)\n  (track_count): True\n)\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.BaseState.value","title":"karbonn.metric.state.BaseState.value  <code>abstractmethod</code>","text":"<pre><code>value(\n    prefix: str = \"\", suffix: str = \"\"\n) -&gt; dict[str, float]\n</code></pre> <p>Compute the metrics given the current state.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in thenreturned dictionary.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The metric values.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 2.5,\n 'error_min': 0.0,\n 'error_max': 5.0,\n 'error_sum': 15.0,\n 'error_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.ErrorState","title":"karbonn.metric.state.ErrorState","text":"<p>               Bases: <code>BaseState</code></p> <p>Implement a metric state to capture some metrics about the errors.</p> <p>This state has a constant space complexity.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>ScalableTensorTracker | None</code> <p>The value tracker.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state\nErrorState(\n  (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n  (track_count): True\n)\n&gt;&gt;&gt; state.get_records(\"error_\")\n(MinScalarRecord(name=error_mean, max_size=10, size=0),\n MinScalarRecord(name=error_min, max_size=10, size=0),\n MinScalarRecord(name=error_max, max_size=10, size=0),\n MinScalarRecord(name=error_sum, max_size=10, size=0))\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 2.5,\n 'error_min': 0.0,\n 'error_max': 5.0,\n 'error_sum': 15.0,\n 'error_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.ErrorState.update","title":"karbonn.metric.state.ErrorState.update","text":"<pre><code>update(error: Tensor) -&gt; None\n</code></pre> <p>Update the metric state with a new tensor of errors.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Tensor</code> <p>A tensor of errors.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ErrorState\n&gt;&gt;&gt; state = ErrorState()\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 2.5,\n 'error_min': 0.0,\n 'error_max': 5.0,\n 'error_sum': 15.0,\n 'error_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.ExtendedAccuracyState","title":"karbonn.metric.state.ExtendedAccuracyState","text":"<p>               Bases: <code>BaseState</code></p> <p>Implement a metric state to compute the accuracy and other metrics.</p> <p>This state has a constant space complexity.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>MeanTensorTracker | None</code> <p>The mean value tracker.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ExtendedAccuracyState\n&gt;&gt;&gt; state = ExtendedAccuracyState()\n&gt;&gt;&gt; state\nExtendedAccuracyState(\n  (tracker): MeanTensorTracker(count=0, total=0.0)\n  (track_count): True\n)\n&gt;&gt;&gt; state.get_records()\n(MaxScalarRecord(name=accuracy, max_size=10, size=0),\n MinScalarRecord(name=error, max_size=10, size=0),\n MaxScalarRecord(name=count_correct, max_size=10, size=0),\n MinScalarRecord(name=count_incorrect, max_size=10, size=0))\n&gt;&gt;&gt; state.update(torch.eye(4))\n&gt;&gt;&gt; state.value()\n{'accuracy': 0.25,\n 'error': 0.75,\n 'count_correct': 4,\n 'count_incorrect': 12,\n 'count': 16}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.ExtendedAccuracyState.update","title":"karbonn.metric.state.ExtendedAccuracyState.update","text":"<pre><code>update(correct: Tensor) -&gt; None\n</code></pre> <p>Update the metric state with a new tensor of errors.</p> <p>Parameters:</p> Name Type Description Default <code>correct</code> <code>Tensor</code> <p>A tensor that indicates the correct predictions. <code>1</code> indicates a correct prediction and <code>0</code> indicates a bad prediction.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ExtendedAccuracyState\n&gt;&gt;&gt; state = ExtendedAccuracyState()\n&gt;&gt;&gt; state.update(torch.eye(4))\n&gt;&gt;&gt; state.value()\n{'accuracy': 0.25,\n 'error': 0.75,\n 'count_correct': 4,\n 'count_incorrect': 12,\n 'count': 16}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.ExtendedErrorState","title":"karbonn.metric.state.ExtendedErrorState","text":"<p>               Bases: <code>BaseState</code></p> <p>Implement a metric state to capture some metrics about the errors.</p> <p>This state stores all the error values, so it does not scale to large datasets. This state has a linear space complexity.</p> <p>Parameters:</p> Name Type Description Default <code>quantiles</code> <code>Tensor | Sequence[float]</code> <p>The quantile values to evaluate.</p> <code>()</code> <code>tracker</code> <code>TensorTracker | None</code> <p>The value tracker.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ExtendedErrorState\n&gt;&gt;&gt; state = ExtendedErrorState(quantiles=[0.5, 0.9])\n&gt;&gt;&gt; state\nExtendedErrorState(\n  (quantiles): tensor([0.5000, 0.9000])\n  (tracker): TensorTracker(count=0)\n  (track_count): True\n)\n&gt;&gt;&gt; state.get_records(\"error_\")\n(MinScalarRecord(name=error_mean, max_size=10, size=0),\n MinScalarRecord(name=error_median, max_size=10, size=0),\n MinScalarRecord(name=error_min, max_size=10, size=0),\n MinScalarRecord(name=error_max, max_size=10, size=0),\n MinScalarRecord(name=error_sum, max_size=10, size=0),\n MinScalarRecord(name=error_quantile_0.5, max_size=10, size=0),\n MinScalarRecord(name=error_quantile_0.9, max_size=10, size=0))\n&gt;&gt;&gt; state.update(torch.arange(11))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 5.0,\n 'error_median': 5,\n 'error_min': 0,\n 'error_max': 10,\n 'error_sum': 55,\n 'error_std': 3.316...,\n 'error_quantile_0.5': 5.0,\n 'error_quantile_0.9': 9.0,\n 'error_count': 11}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.ExtendedErrorState.update","title":"karbonn.metric.state.ExtendedErrorState.update","text":"<pre><code>update(error: Tensor) -&gt; None\n</code></pre> <p>Update the metric state with a new tensor of errors.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Tensor</code> <p>A tensor of errors.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import ExtendedErrorState\n&gt;&gt;&gt; state = ExtendedErrorState(quantiles=[0.5, 0.9])\n&gt;&gt;&gt; state.update(torch.arange(11))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 5.0,\n 'error_median': 5,\n 'error_min': 0,\n 'error_max': 10,\n 'error_sum': 55,\n 'error_std': 3.316...,\n 'error_quantile_0.5': 5.0,\n 'error_quantile_0.9': 9.0,\n 'error_count': 11}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.MeanErrorState","title":"karbonn.metric.state.MeanErrorState","text":"<p>               Bases: <code>BaseState</code></p> <p>Implement a metric state to capture the mean error value.</p> <p>This state has a constant space complexity.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>MeanTensorTracker | None</code> <p>The mean value tracker.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import MeanErrorState\n&gt;&gt;&gt; state = MeanErrorState()\n&gt;&gt;&gt; state\nMeanErrorState(\n  (tracker): MeanTensorTracker(count=0, total=0.0)\n  (track_count): True\n)\n&gt;&gt;&gt; state.get_records(\"error_\")\n(MinScalarRecord(name=error_mean, max_size=10, size=0),)\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 2.5, 'error_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.MeanErrorState.update","title":"karbonn.metric.state.MeanErrorState.update","text":"<pre><code>update(error: Tensor) -&gt; None\n</code></pre> <p>Update the metric state with a new tensor of errors.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Tensor</code> <p>A tensor of errors.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import MeanErrorState\n&gt;&gt;&gt; state = MeanErrorState()\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 2.5, 'error_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.NormalizedMeanSquaredErrorState","title":"karbonn.metric.state.NormalizedMeanSquaredErrorState","text":"<p>               Bases: <code>BaseState</code></p> <p>Implement a metric state to capture the normalized mean squared error value.</p> <p>This state has a constant space complexity.</p> <p>Parameters:</p> Name Type Description Default <code>squared_errors</code> <code>MeanTensorTracker | None</code> <p>The value tracker for squared errors.</p> <code>None</code> <code>squared_targets</code> <code>MeanTensorTracker | None</code> <p>The value tracker for squared targets.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import NormalizedMeanSquaredErrorState\n&gt;&gt;&gt; state = NormalizedMeanSquaredErrorState()\n&gt;&gt;&gt; state\nNormalizedMeanSquaredErrorState(\n  (squared_errors): MeanTensorTracker(count=0, total=0.0)\n  (squared_targets): MeanTensorTracker(count=0, total=0.0)\n  (track_count): True\n)\n&gt;&gt;&gt; state.get_records(\"nmse_\")\n(MinScalarRecord(name=nmse_mean, max_size=10, size=0),)\n&gt;&gt;&gt; state.update(torch.arange(6), torch.ones(6))\n&gt;&gt;&gt; state.value(\"nmse_\")\n{'nmse_mean': 9.166..., 'nmse_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.NormalizedMeanSquaredErrorState.update","title":"karbonn.metric.state.NormalizedMeanSquaredErrorState.update","text":"<pre><code>update(error: Tensor, target: Tensor) -&gt; None\n</code></pre> <p>Update the metric state with a new tensor of errors.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Tensor</code> <p>A tensor of errors.</p> required <code>target</code> <code>Tensor</code> <p>A tensor with the target values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import NormalizedMeanSquaredErrorState\n&gt;&gt;&gt; state = NormalizedMeanSquaredErrorState()\n&gt;&gt;&gt; state.update(torch.arange(6), torch.ones(6))\n&gt;&gt;&gt; state.value(\"nmse_\")\n{'nmse_mean': 9.166..., 'nmse_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.RootMeanErrorState","title":"karbonn.metric.state.RootMeanErrorState","text":"<p>               Bases: <code>BaseState</code></p> <p>Implement a metric state to capture the root mean error value.</p> <p>This state has a constant space complexity.</p> <p>Parameters:</p> Name Type Description Default <code>tracker</code> <code>MeanTensorTracker | None</code> <p>The mean value tracker.</p> <code>None</code> <code>track_count</code> <code>bool</code> <p>If <code>True</code>, the state tracks and returns the number of predictions.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import RootMeanErrorState\n&gt;&gt;&gt; state = RootMeanErrorState()\n&gt;&gt;&gt; state\nRootMeanErrorState(\n  (tracker): MeanTensorTracker(count=0, total=0.0)\n  (track_count): True\n)\n&gt;&gt;&gt; state.get_records(\"error_\")\n(MinScalarRecord(name=error_mean, max_size=10, size=0),)\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 1.581..., 'error_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.RootMeanErrorState.update","title":"karbonn.metric.state.RootMeanErrorState.update","text":"<pre><code>update(error: Tensor) -&gt; None\n</code></pre> <p>Update the metric state with a new tensor of errors.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Tensor</code> <p>A tensor of errors.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.metric.state import RootMeanErrorState\n&gt;&gt;&gt; state = RootMeanErrorState()\n&gt;&gt;&gt; state.update(torch.arange(6))\n&gt;&gt;&gt; state.value(\"error_\")\n{'error_mean': 1.581..., 'error_count': 6}\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.is_state_config","title":"karbonn.metric.state.is_state_config","text":"<pre><code>is_state_config(config: dict) -&gt; bool\n</code></pre> <p>Indicate if the input configuration is a configuration for a <code>BaseState</code>.</p> <p>This function only checks if the value of the key  <code>_target_</code> is valid. It does not check the other values. If <code>_target_</code> indicates a function, the returned type hint is used to check the class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the input configuration is a configuration for a <code>BaseState</code> object, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.metric.state import is_state_config\n&gt;&gt;&gt; is_state_config({\"_target_\": \"karbonn.metric.state.ErrorState\"})\nTrue\n</code></pre>"},{"location":"refs/metric/#karbonn.metric.state.setup_state","title":"karbonn.metric.state.setup_state","text":"<pre><code>setup_state(state: BaseState | dict) -&gt; BaseState\n</code></pre> <p>Set up a <code>BaseState</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>BaseState | dict</code> <p>The state or its configuration.</p> required <p>Returns:</p> Type Description <code>BaseState</code> <p>The instantiated <code>BaseState</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.metric.state import setup_state\n&gt;&gt;&gt; state = setup_state({\"_target_\": \"karbonn.metric.state.ErrorState\"})\n&gt;&gt;&gt; state\nErrorState(\n  (tracker): ScalableTensorTracker(count=0, total=0.0, min_value=inf, max_value=-inf)\n  (track_count): True\n)\n</code></pre>"},{"location":"refs/modules/","title":"karbonn.modules","text":""},{"location":"refs/modules/#karbonn.modules","title":"karbonn.modules","text":"<p>Contains some modules.</p>"},{"location":"refs/modules/#karbonn.modules.Asinh","title":"karbonn.modules.Asinh","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the inverse hyperbolic sine (arcsinh) of the elements.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Asinh\n&gt;&gt;&gt; m = Asinh()\n&gt;&gt;&gt; m\nAsinh()\n&gt;&gt;&gt; out = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; out\ntensor([[-0.8814,  0.0000,  0.8814],\n        [-1.4436,  1.4436,  2.0947]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.AsinhCosSinNumericalEncoder","title":"karbonn.modules.AsinhCosSinNumericalEncoder","text":"<p>               Bases: <code>CosSinNumericalEncoder</code></p> <p>Extension of <code>CosSinNumericalEncoder</code> with an additional feature built using the inverse hyperbolic sine (arcsinh).</p> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>Tensor</code> <p>The initial frequency values. This input should be a tensor of shape <code>(n_features, feature_size // 2)</code> or <code>(feature_size // 2,)</code>.</p> required <code>phase_shift</code> <code>Tensor</code> <p>The initial phase-shift values. This input should be a tensor of shape <code>(n_features, feature_size // 2)</code> or <code>(feature_size // 2,)</code>.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the frequencies and phase-shift parameters are learnable, otherwise they are frozen.</p> <code>False</code> Shape <ul> <li>Input: <code>(*, n_features)</code>, where <code>*</code> means any number of     dimensions.</li> <li>Output: <code>(*, n_features, feature_size + 1)</code>,  where <code>*</code>     has the same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import AsinhCosSinNumericalEncoder\n&gt;&gt;&gt; # Example with 1 feature\n&gt;&gt;&gt; m = AsinhCosSinNumericalEncoder(\n...     frequency=torch.tensor([[1.0, 2.0, 4.0]]),\n...     phase_shift=torch.zeros(1, 3),\n... )\n&gt;&gt;&gt; m\nAsinhCosSinNumericalEncoder(frequency=(1, 6), phase_shift=(1, 6), learnable=False)\n&gt;&gt;&gt; out = m(torch.tensor([[0.0], [1.0], [2.0], [3.0]]))\n&gt;&gt;&gt; out\ntensor([[[ 0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000,  0.0000]],\n        [[ 0.8415,  0.9093, -0.7568,  0.5403, -0.4161, -0.6536,  0.8814]],\n        [[ 0.9093, -0.7568,  0.9894, -0.4161, -0.6536, -0.1455,  1.4436]],\n        [[ 0.1411, -0.2794, -0.5366, -0.9900,  0.9602,  0.8439,  1.8184]]])\n&gt;&gt;&gt; # Example with 2 features\n&gt;&gt;&gt; m = AsinhCosSinNumericalEncoder(\n...     frequency=torch.tensor([[1.0, 2.0, 4.0], [2.0, 4.0, 6.0]]),\n...     phase_shift=torch.zeros(2, 3),\n... )\n&gt;&gt;&gt; m\nAsinhCosSinNumericalEncoder(frequency=(2, 6), phase_shift=(2, 6), learnable=False)\n&gt;&gt;&gt; out = m(torch.tensor([[0.0, 3.0], [1.0, 2.0], [2.0, 1.0], [3.0, 0.0]]))\n&gt;&gt;&gt; out\ntensor([[[ 0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000,  0.0000],\n         [-0.2794, -0.5366, -0.7510,  0.9602,  0.8439,  0.6603,  1.8184]],\n        [[ 0.8415,  0.9093, -0.7568,  0.5403, -0.4161, -0.6536,  0.8814],\n         [-0.7568,  0.9894, -0.5366, -0.6536, -0.1455,  0.8439,  1.4436]],\n        [[ 0.9093, -0.7568,  0.9894, -0.4161, -0.6536, -0.1455,  1.4436],\n         [ 0.9093, -0.7568, -0.2794, -0.4161, -0.6536,  0.9602,  0.8814]],\n        [[ 0.1411, -0.2794, -0.5366, -0.9900,  0.9602,  0.8439,  1.8184],\n         [ 0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000,  0.0000]]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.AsinhCosSinNumericalEncoder.output_size","title":"karbonn.modules.AsinhCosSinNumericalEncoder.output_size  <code>property</code>","text":"<pre><code>output_size: int\n</code></pre> <p>Return the output feature size.</p>"},{"location":"refs/modules/#karbonn.modules.AsinhMSELoss","title":"karbonn.modules.AsinhMSELoss","text":"<p>               Bases: <code>Module</code></p> <p>Implement a loss module that computes the mean squared error (MSE) on the inverse hyperbolic sine (asinh) transformed predictions and targets.</p> <p>It is a generalization of mean squared logarithmic error (MSLE) that works for real values. The <code>asinh</code> transformation is used instead of <code>log1p</code> because <code>asinh</code> works on negative values.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import AsinhMSELoss\n&gt;&gt;&gt; criterion = AsinhMSELoss()\n&gt;&gt;&gt; criterion\nAsinhMSELoss(reduction=mean)\n&gt;&gt;&gt; loss = criterion(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MseLossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.AsinhNumericalEncoder","title":"karbonn.modules.AsinhNumericalEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Implement a numerical encoder using the inverse hyperbolic sine (asinh).</p> <p>Parameters:</p> Name Type Description Default <code>scale</code> <code>Tensor</code> <p>The initial scale values. This input should be a tensor of shape <code>(n_features, feature_size)</code> or <code>(feature_size,)</code>.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the scales are learnable, otherwise they are frozen.</p> <code>False</code> Shape <ul> <li>Input: <code>(*, n_features)</code>, where <code>*</code> means any number of     dimensions.</li> <li>Output: <code>(*, n_features, feature_size)</code>,  where <code>*</code> has     the same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import AsinhNumericalEncoder\n&gt;&gt;&gt; # Example with 1 feature\n&gt;&gt;&gt; m = AsinhNumericalEncoder(scale=torch.tensor([[1.0, 2.0, 4.0]]))\n&gt;&gt;&gt; m\nAsinhNumericalEncoder(scale=(1, 3), learnable=False)\n&gt;&gt;&gt; out = m(torch.tensor([[0.0], [1.0], [2.0], [3.0]]))\n&gt;&gt;&gt; out\ntensor([[[0.0000, 0.0000, 0.0000]],\n        [[0.8814, 1.4436, 2.0947]],\n        [[1.4436, 2.0947, 2.7765]],\n        [[1.8184, 2.4918, 3.1798]]])\n&gt;&gt;&gt; # Example with 2 features\n&gt;&gt;&gt; m = AsinhNumericalEncoder(scale=torch.tensor([[1.0, 2.0, 4.0], [1.0, 3.0, 6.0]]))\n&gt;&gt;&gt; m\nAsinhNumericalEncoder(scale=(2, 3), learnable=False)\n&gt;&gt;&gt; out = m(torch.tensor([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]))\n&gt;&gt;&gt; out\ntensor([[[0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000]],\n        [[0.8814, 1.4436, 2.0947], [0.8814, 1.8184, 2.4918]],\n        [[1.4436, 2.0947, 2.7765], [1.4436, 2.4918, 3.1798]],\n        [[1.8184, 2.4918, 3.1798], [1.8184, 2.8934, 3.5843]]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.AsinhNumericalEncoder.input_size","title":"karbonn.modules.AsinhNumericalEncoder.input_size  <code>property</code>","text":"<pre><code>input_size: int\n</code></pre> <p>Return the input feature size.</p>"},{"location":"refs/modules/#karbonn.modules.AsinhNumericalEncoder.output_size","title":"karbonn.modules.AsinhNumericalEncoder.output_size  <code>property</code>","text":"<pre><code>output_size: int\n</code></pre> <p>Return the output feature size.</p>"},{"location":"refs/modules/#karbonn.modules.AsinhSmoothL1Loss","title":"karbonn.modules.AsinhSmoothL1Loss","text":"<p>               Bases: <code>Module</code></p> <p>Implement a loss module that computes the smooth L1 loss on the inverse hyperbolic sine (asinh) transformed predictions and targets.</p> <p>It is a generalization of mean squared logarithmic error (MSLE) that works for real values. The <code>asinh</code> transformation is used instead of <code>log1p</code> because <code>asinh</code> works on negative values.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>beta</code> <code>float</code> <p>The threshold at which to change between L1 and L2 loss. The value must be non-negative.</p> <code>1.0</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import AsinhSmoothL1Loss\n&gt;&gt;&gt; criterion = AsinhSmoothL1Loss()\n&gt;&gt;&gt; criterion\nAsinhSmoothL1Loss(reduction=mean, beta=1.0)\n&gt;&gt;&gt; loss = criterion(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;SmoothL1LossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.AverageFusion","title":"karbonn.modules.AverageFusion","text":"<p>               Bases: <code>SumFusion</code></p> <p>Implement a layer to average the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import AverageFusion\n&gt;&gt;&gt; module = AverageFusion()\n&gt;&gt;&gt; module\nAverageFusion(normalized=True)\n&gt;&gt;&gt; x1 = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n&gt;&gt;&gt; x2 = torch.tensor([[12.0, 13.0, 14.0], [15.0, 16.0, 17.0]], requires_grad=True)\n&gt;&gt;&gt; out = module(x1, x2)\n&gt;&gt;&gt; out\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]], grad_fn=&lt;DivBackward0&gt;)\n&gt;&gt;&gt; out.mean().backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.BinaryFocalLoss","title":"karbonn.modules.BinaryFocalLoss","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of the binary Focal Loss.</p> <p>Based on \"Focal Loss for Dense Object Detection\" (https://arxiv.org/pdf/1708.02002.pdf)</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Module | dict</code> <p>The binary cross entropy layer or another equivalent layer. To be used as in the original paper, this loss should not use reducton as the reduction is done in this class.</p> required <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>.</p> <code>0.5</code> <code>gamma</code> <code>float</code> <p>The focusing parameter, which must be positive (<code>&gt;=0</code>).</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>The reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Target: <code>(*)</code>, same shape as the input.</li> <li>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code>(*)</code>, same   shape as input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import BinaryFocalLoss\n&gt;&gt;&gt; criterion = BinaryFocalLoss(nn.BCEWithLogitsLoss(reduction=\"none\"))\n&gt;&gt;&gt; criterion\nBinaryFocalLoss(\n  alpha=0.5, gamma=2.0, reduction=mean\n  (loss): BCEWithLogitsLoss()\n)\n&gt;&gt;&gt; input = torch.randn(3, 2, requires_grad=True)\n&gt;&gt;&gt; target = torch.rand(3, 2)\n&gt;&gt;&gt; loss = criterion(input, target)\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.BinaryFocalLoss.forward","title":"karbonn.modules.BinaryFocalLoss.forward","text":"<pre><code>forward(prediction: Tensor, target: Tensor) -&gt; Tensor\n</code></pre> <p>Compute the binary Focal Loss.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>Tensor</code> <p>The predicted probabilities or the un-normalized scores.</p> required <code>target</code> <code>Tensor</code> <p>The targets where <code>1</code> (resp. <code>0</code>) means a positive (resp. negative) example.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p><code>torch.Tensor</code> of type float: The loss value(s). The shape of the tensor depends on the reduction. If the reduction is <code>mean</code> or <code>sum</code>, the tensor has a single scalar value. If the reduction is <code>none</code>, the shape of the tensor is the same that the inputs.</p>"},{"location":"refs/modules/#karbonn.modules.Clamp","title":"karbonn.modules.Clamp","text":"<p>               Bases: <code>Module</code></p> <p>Implement a module to clamp all elements in input into the range <code>[min, max]</code>.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float | None</code> <p>The lower-bound of the range to be clamped to. <code>None</code> means there is no minimum value.</p> <code>-1.0</code> <code>max</code> <code>float | None</code> <p>The upper-bound of the range to be clamped to. <code>None</code> means there is no maximum value.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Clamp\n&gt;&gt;&gt; m = Clamp(min=-1, max=2)\n&gt;&gt;&gt; m\nClamp(min=-1, max=2)\n&gt;&gt;&gt; out = m(torch.tensor([[-2.0, -1.0, 0.0], [1.0, 2.0, 3.0]]))\n&gt;&gt;&gt; out\ntensor([[-1., -1.,  0.], [ 1.,  2.,  2.]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ConcatFusion","title":"karbonn.modules.ConcatFusion","text":"<p>               Bases: <code>Module</code></p> <p>Implement a module to concatenate inputs.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>The fusion dimension. <code>-1</code> means the last dimension.</p> <code>-1</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ConcatFusion\n&gt;&gt;&gt; module = ConcatFusion()\n&gt;&gt;&gt; module\nConcatFusion(dim=-1)\n&gt;&gt;&gt; x1 = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n&gt;&gt;&gt; x2 = torch.tensor([[12.0, 13.0, 14.0], [15.0, 16.0, 17.0]], requires_grad=True)\n&gt;&gt;&gt; out = module(x1, x2)\n&gt;&gt;&gt; out\ntensor([[ 2.,  3.,  4., 12., 13., 14.],\n        [ 5.,  6.,  7., 15., 16., 17.]], grad_fn=&lt;CatBackward0&gt;)\n&gt;&gt;&gt; out.mean().backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder","title":"karbonn.modules.CosSinNumericalEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Implement a frequency/phase-shift numerical encoder where the periodic functions are cosine and sine.</p> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>Tensor</code> <p>The initial frequency values. This input should be a tensor of shape <code>(n_features, feature_size // 2)</code> or <code>(feature_size // 2,)</code>.</p> required <code>phase_shift</code> <code>Tensor</code> <p>The initial phase-shift values. This input should be a tensor of shape <code>(n_features, feature_size // 2)</code> or <code>(feature_size // 2,)</code>.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the frequencies and phase-shift parameters are learnable, otherwise they are frozen.</p> <code>False</code> Shape <ul> <li>Input: <code>(*, n_features)</code>, where <code>*</code> means any number of     dimensions.</li> <li>Output: <code>(*, n_features, feature_size)</code>,  where <code>*</code> has     the same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import CosSinNumericalEncoder\n&gt;&gt;&gt; # Example with 1 feature\n&gt;&gt;&gt; m = CosSinNumericalEncoder(\n...     frequency=torch.tensor([[1.0, 2.0, 4.0]]),\n...     phase_shift=torch.zeros(1, 3),\n... )\n&gt;&gt;&gt; m\nCosSinNumericalEncoder(frequency=(1, 6), phase_shift=(1, 6), learnable=False)\n&gt;&gt;&gt; out = m(torch.tensor([[0.0], [1.0], [2.0], [3.0]]))\n&gt;&gt;&gt; out\ntensor([[[ 0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000]],\n        [[ 0.8415,  0.9093, -0.7568,  0.5403, -0.4161, -0.6536]],\n        [[ 0.9093, -0.7568,  0.9894, -0.4161, -0.6536, -0.1455]],\n        [[ 0.1411, -0.2794, -0.5366, -0.9900,  0.9602,  0.8439]]])\n&gt;&gt;&gt; # Example with 2 features\n&gt;&gt;&gt; m = CosSinNumericalEncoder(\n...     frequency=torch.tensor([[1.0, 2.0, 4.0], [2.0, 4.0, 6.0]]),\n...     phase_shift=torch.zeros(2, 3),\n... )\n&gt;&gt;&gt; m\nCosSinNumericalEncoder(frequency=(2, 6), phase_shift=(2, 6), learnable=False)\n&gt;&gt;&gt; out = m(torch.tensor([[0.0, 3.0], [1.0, 2.0], [2.0, 1.0], [3.0, 0.0]]))\n&gt;&gt;&gt; out\ntensor([[[ 0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000],\n         [-0.2794, -0.5366, -0.7510,  0.9602,  0.8439,  0.6603]],\n        [[ 0.8415,  0.9093, -0.7568,  0.5403, -0.4161, -0.6536],\n         [-0.7568,  0.9894, -0.5366, -0.6536, -0.1455,  0.8439]],\n        [[ 0.9093, -0.7568,  0.9894, -0.4161, -0.6536, -0.1455],\n         [ 0.9093, -0.7568, -0.2794, -0.4161, -0.6536,  0.9602]],\n        [[ 0.1411, -0.2794, -0.5366, -0.9900,  0.9602,  0.8439],\n         [ 0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  1.0000]]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder.input_size","title":"karbonn.modules.CosSinNumericalEncoder.input_size  <code>property</code>","text":"<pre><code>input_size: int\n</code></pre> <p>Return the input feature size.</p>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder.output_size","title":"karbonn.modules.CosSinNumericalEncoder.output_size  <code>property</code>","text":"<pre><code>output_size: int\n</code></pre> <p>Return the output feature size.</p>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder.create_linspace_frequency","title":"karbonn.modules.CosSinNumericalEncoder.create_linspace_frequency  <code>classmethod</code>","text":"<pre><code>create_linspace_frequency(\n    num_frequencies: int,\n    min_frequency: float,\n    max_frequency: float,\n    learnable: bool = False,\n) -&gt; CosSinNumericalEncoder\n</code></pre> <p>Create a `CosSinNumericalEncoder`` where the frequencies are evenly spaced in a frequency range.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>The number of frequencies.</p> required <code>min_frequency</code> <code>float</code> <p>The minimum frequency.</p> required <code>max_frequency</code> <code>float</code> <p>The maximum frequency.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the parameters are learnable, otherwise they are frozen.</p> <code>False</code> <p>Returns:</p> Type Description <code>CosSinNumericalEncoder</code> <p>An instantiated <code>CosSinNumericalEncoder</code> where the frequencies are evenly spaced in a frequency range.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import CosSinNumericalEncoder\n&gt;&gt;&gt; m = CosSinNumericalEncoder.create_linspace_frequency(\n...     num_frequencies=5, min_frequency=0.1, max_frequency=1.0\n... )\n&gt;&gt;&gt; m\nCosSinNumericalEncoder(frequency=(1, 10), phase_shift=(1, 10), learnable=False)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder.create_linspace_value_range","title":"karbonn.modules.CosSinNumericalEncoder.create_linspace_value_range  <code>classmethod</code>","text":"<pre><code>create_linspace_value_range(\n    num_frequencies: int,\n    min_abs_value: float,\n    max_abs_value: float,\n    learnable: bool = False,\n) -&gt; CosSinNumericalEncoder\n</code></pre> <p>Create a <code>CosSinNumericalEncoder</code> where the frequencies are evenly spaced given a value range.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>The number of frequencies.</p> required <code>min_abs_value</code> <code>float</code> <p>The minimum absolute value to encode.</p> required <code>max_abs_value</code> <code>float</code> <p>The maximum absolute value to encoder.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the parameters are learnable, otherwise they are frozen.</p> <code>False</code> <p>Returns:</p> Type Description <code>CosSinNumericalEncoder</code> <p>An instantiated <code>CosSinNumericalEncoder</code> where the frequencies are evenly spaced.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import CosSinNumericalEncoder\n&gt;&gt;&gt; m = CosSinNumericalEncoder.create_linspace_value_range(\n...     num_frequencies=5, min_abs_value=0.1, max_abs_value=1.0\n... )\n&gt;&gt;&gt; m\nCosSinNumericalEncoder(frequency=(1, 10), phase_shift=(1, 10), learnable=False)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder.create_logspace_frequency","title":"karbonn.modules.CosSinNumericalEncoder.create_logspace_frequency  <code>classmethod</code>","text":"<pre><code>create_logspace_frequency(\n    num_frequencies: int,\n    min_frequency: float,\n    max_frequency: float,\n    learnable: bool = False,\n) -&gt; CosSinNumericalEncoder\n</code></pre> <p>Create a <code>CosSinNumericalEncoder</code> where the frequencies are evenly spaced in the log space in a frequency range.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>The number of frequencies.</p> required <code>min_frequency</code> <code>float</code> <p>The minimum frequency.</p> required <code>max_frequency</code> <code>float</code> <p>The maximum frequency.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the parameters are learnable, otherwise they are frozen.</p> <code>False</code> <p>Returns:</p> Type Description <code>CosSinNumericalEncoder</code> <p>An instantiated <code>CosSinNumericalEncoder</code> where the frequencies are evenly spaced in the log space in a frequency range.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import CosSinNumericalEncoder\n&gt;&gt;&gt; m = CosSinNumericalEncoder.create_logspace_frequency(\n...     num_frequencies=5, min_frequency=0.1, max_frequency=1.0\n... )\n&gt;&gt;&gt; m\nCosSinNumericalEncoder(frequency=(1, 10), phase_shift=(1, 10), learnable=False)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder.create_logspace_value_range","title":"karbonn.modules.CosSinNumericalEncoder.create_logspace_value_range  <code>classmethod</code>","text":"<pre><code>create_logspace_value_range(\n    num_frequencies: int,\n    min_abs_value: float,\n    max_abs_value: float,\n    learnable: bool = False,\n) -&gt; CosSinNumericalEncoder\n</code></pre> <p>Create a <code>CosSinNumericalEncoder</code> where the frequencies are evenly spaced in the log space given a value range.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>The number of frequencies.</p> required <code>min_abs_value</code> <code>float</code> <p>The minimum absolute value to encode.</p> required <code>max_abs_value</code> <code>float</code> <p>The maximum absolute value to encoder.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the parameters are learnable, otherwise they are frozen.</p> <code>False</code> <p>Returns:</p> Type Description <code>CosSinNumericalEncoder</code> <p>An instantiated <code>CosSinNumericalEncoder</code> where the frequencies are evenly spaced in the log space.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import CosSinNumericalEncoder\n&gt;&gt;&gt; m = CosSinNumericalEncoder.create_logspace_value_range(\n...     num_frequencies=5, min_abs_value=0.1, max_abs_value=1.0\n... )\n&gt;&gt;&gt; m\nCosSinNumericalEncoder(frequency=(1, 10), phase_shift=(1, 10), learnable=False)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder.create_rand_frequency","title":"karbonn.modules.CosSinNumericalEncoder.create_rand_frequency  <code>classmethod</code>","text":"<pre><code>create_rand_frequency(\n    num_frequencies: int,\n    min_frequency: float,\n    max_frequency: float,\n    learnable: bool = False,\n) -&gt; CosSinNumericalEncoder\n</code></pre> <p>Create a <code>CosSinNumericalEncoder</code> where the frequencies are uniformly initialized in a frequency range.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>The number of frequencies.</p> required <code>min_frequency</code> <code>float</code> <p>The minimum frequency.</p> required <code>max_frequency</code> <code>float</code> <p>The maximum frequency.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the parameters are learnable, otherwise they are frozen.</p> <code>False</code> <p>Returns:</p> Type Description <code>CosSinNumericalEncoder</code> <p>An instantiated <code>CosSinNumericalEncoder</code> where the frequencies are uniformly initialized in a frequency range.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import CosSinNumericalEncoder\n&gt;&gt;&gt; m = CosSinNumericalEncoder.create_rand_frequency(\n...     num_frequencies=5, min_frequency=0.1, max_frequency=1.0\n... )\n&gt;&gt;&gt; m\nCosSinNumericalEncoder(frequency=(1, 10), phase_shift=(1, 10), learnable=False)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.CosSinNumericalEncoder.create_rand_value_range","title":"karbonn.modules.CosSinNumericalEncoder.create_rand_value_range  <code>classmethod</code>","text":"<pre><code>create_rand_value_range(\n    num_frequencies: int,\n    min_abs_value: float,\n    max_abs_value: float,\n    learnable: bool = False,\n) -&gt; CosSinNumericalEncoder\n</code></pre> <p>Create a <code>CosSinNumericalEncoder</code> where the frequencies are uniformly initialized for a given value range.</p> <p>Parameters:</p> Name Type Description Default <code>num_frequencies</code> <code>int</code> <p>The number of frequencies.</p> required <code>min_abs_value</code> <code>float</code> <p>The minimum absolute value to encode.</p> required <code>max_abs_value</code> <code>float</code> <p>The maximum absolute value to encode.</p> required <code>learnable</code> <code>bool</code> <p>If <code>True</code> the parameters are learnable, otherwise they are frozen.</p> <code>False</code> <p>Returns:</p> Type Description <code>CosSinNumericalEncoder</code> <p>An instantiated <code>CosSinNumericalEncoder</code> where the frequencies are uniformly initialized for a given value range.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import CosSinNumericalEncoder\n&gt;&gt;&gt; m = CosSinNumericalEncoder.create_rand_value_range(\n...     num_frequencies=5, min_abs_value=0.1, max_abs_value=1.0\n... )\n&gt;&gt;&gt; m\nCosSinNumericalEncoder(frequency=(1, 10), phase_shift=(1, 10), learnable=False)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ExU","title":"karbonn.modules.ExU","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of the exp-centered (ExU) layer.</p> <p>This layer was proposed in the following paper:</p> <pre><code>Neural Additive Models: Interpretable Machine Learning with\nNeural Nets.\nAgarwal R., Melnick L., Frosst N., Zhang X., Lengerich B.,\nCaruana R., Hinton G.\nNeurIPS 2021. (https://arxiv.org/pdf/2004.13912.pdf)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>The size of each input sample.</p> required <code>out_features</code> <code>int</code> <p>The size of each output sample.</p> required <code>bias</code> <code>bool</code> <p>If set to <code>False</code>, the layer will not learn an additive bias.</p> <code>True</code> <code>device</code> <code>device | None</code> <p>The device where to initialize the layer's parameters.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>The data type of the layer's parameters.</p> <code>None</code> Shape <ul> <li>Input: <code>(*, in_features)</code>, where <code>*</code> means any number of     dimensions, including none.</li> <li>Output: <code>(*, out_features)</code>, where <code>*</code> is the same shape     as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ExU\n&gt;&gt;&gt; m = ExU(4, 6)\n&gt;&gt;&gt; m\nExU(in_features=4, out_features=6, bias=True)\n&gt;&gt;&gt; out = m(torch.rand(6, 4))\n&gt;&gt;&gt; out\ntensor([[...]], grad_fn=&lt;MmBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ExU.reset_parameters","title":"karbonn.modules.ExU.reset_parameters","text":"<pre><code>reset_parameters() -&gt; None\n</code></pre> <p>Reset the parameters.</p> <p>As indicated in page 4 of the paper, the weights are initialed using a normal distribution <code>N(4.0; 0.5)</code>. The biases are initialized to <code>0</code></p>"},{"location":"refs/modules/#karbonn.modules.Exp","title":"karbonn.modules.Exp","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the input.</p> <p>This module is equivalent to  <code>exp(input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Exp\n&gt;&gt;&gt; m = Exp()\n&gt;&gt;&gt; m\nExp()\n&gt;&gt;&gt; out = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 3.0]]))\n&gt;&gt;&gt; out\ntensor([[ 0.3679,  1.0000,  2.7183],\n        [ 0.1353,  7.3891, 20.0855]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ExpSin","title":"karbonn.modules.ExpSin","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the ExpSin activation layer.</p> <p>Formula: <code>exp(-sin(alpha * x))</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ExpSin\n&gt;&gt;&gt; m = ExpSin()\n&gt;&gt;&gt; m\nExpSin(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000, 2.3198, 2.4826, 1.1516],\n        [0.4692, 0.3833, 0.7562, 1.9290]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Expm1","title":"karbonn.modules.Expm1","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the elements minus 1 of input.</p> <p>This module is equivalent to  <code>exp(input) - 1</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Expm1\n&gt;&gt;&gt; m = Expm1()\n&gt;&gt;&gt; m\nExpm1()\n&gt;&gt;&gt; out = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; out\ntensor([[-0.6321,  0.0000,  1.7183],\n        [-0.8647,  6.3891, 53.5981]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Gaussian","title":"karbonn.modules.Gaussian","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Gaussian activation layer.</p> <p>Formula: <code>exp(-0.5 * x^2 / alpha^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Gaussian\n&gt;&gt;&gt; m = Gaussian()\n&gt;&gt;&gt; m\nGaussian(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000e+00, 6.0653e-01, 1.3534e-01, 1.1109e-02],\n        [3.3546e-04, 3.7267e-06, 1.5230e-08, 2.2897e-11]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.GeneralRobustRegressionLoss","title":"karbonn.modules.GeneralRobustRegressionLoss","text":"<p>               Bases: <code>Module</code></p> <p>Implement the general robust regression loss a.k.a. Barron robust loss.</p> <p>Based on the paper:</p> <pre><code>A General and Adaptive Robust Loss Function\nJonathan T. Barron\nCVPR 2019 (https://arxiv.org/abs/1701.03077)\n</code></pre> Note <p>The \"adaptative\" part of the loss is not implemented in this     function.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The shape parameter that controls the robustness of the loss.</p> <code>2.0</code> <code>scale</code> <code>float</code> <p>The scale parameter that controls the size of the loss's quadratic bowl near 0.</p> <code>1.0</code> <code>max</code> <code>float | None</code> <p>The max value to clip the loss before to compute the reduction. <code>None</code> means no clipping is used.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Target: <code>(*)</code>, same shape as the input.</li> <li>Output: scalar. If <code>reduction</code> is <code>'none'</code>, then <code>(*)</code>, same   shape as input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import GeneralRobustRegressionLoss\n&gt;&gt;&gt; criterion = GeneralRobustRegressionLoss()\n&gt;&gt;&gt; criterion\nGeneralRobustRegressionLoss(alpha=2.0, scale=1.0, max=None, reduction=mean)\n&gt;&gt;&gt; input = torch.randn(3, 2, requires_grad=True)\n&gt;&gt;&gt; target = torch.rand(3, 2, requires_grad=False)\n&gt;&gt;&gt; loss = criterion(input, target)\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Laplacian","title":"karbonn.modules.Laplacian","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Laplacian activation layer.</p> <p>Formula: <code>exp(-|x| / alpha)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Laplacian\n&gt;&gt;&gt; m = Laplacian()\n&gt;&gt;&gt; m\nLaplacian(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000e+00, 3.6788e-01, 1.3534e-01, 4.9787e-02],\n        [1.8316e-02, 6.7379e-03, 2.4788e-03, 9.1188e-04]], grad_fn=&lt;ExpBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Log","title":"karbonn.modules.Log","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the natural logarithm of the input.</p> <p>This module is equivalent to  <code>log(input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Log\n&gt;&gt;&gt; m = Log()\n&gt;&gt;&gt; m\nLog()\n&gt;&gt;&gt; out = m(torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]))\n&gt;&gt;&gt; out\ntensor([[0.0000, 0.6931, 1.0986],\n        [1.3863, 1.6094, 1.7918]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Log1p","title":"karbonn.modules.Log1p","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the natural logarithm of <code>(1 + input)</code>.</p> <p>This module is equivalent to  <code>log(1 + input)</code></p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Log1p\n&gt;&gt;&gt; m = Log1p()\n&gt;&gt;&gt; m\nLog1p()\n&gt;&gt;&gt; out = m(torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0]]))\n&gt;&gt;&gt; out\ntensor([[0.0000, 0.6931, 1.0986],\n        [1.3863, 1.6094, 1.7918]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.MultiQuadratic","title":"karbonn.modules.MultiQuadratic","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Multi Quadratic activation layer.</p> <p>Formula: <code>1 / sqrt(1 + (alpha * x)^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import MultiQuadratic\n&gt;&gt;&gt; m = MultiQuadratic()\n&gt;&gt;&gt; m\nMultiQuadratic(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000, 0.7071, 0.4472, 0.3162],\n        [0.2425, 0.1961, 0.1644, 0.1414]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.MulticlassFlatten","title":"karbonn.modules.MulticlassFlatten","text":"<p>               Bases: <code>Module</code></p> <p>Implement a wrapper to flat the multiclass inputs of a <code>torch.nn.Module</code>.</p> <p>The input prediction tensor shape is <code>(d1, d2, ..., dn, C)</code> and is reshaped to <code>(d1 * d2 * ... * dn, C)</code>. The input target tensor shape is <code>(d1, d2, ..., dn)</code> and is reshaped to <code>(d1 * d2 * ... * dn,)</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import MulticlassFlatten\n&gt;&gt;&gt; m = MulticlassFlatten(torch.nn.CrossEntropyLoss())\n&gt;&gt;&gt; m\nMulticlassFlatten(\n  (module): CrossEntropyLoss()\n)\n&gt;&gt;&gt; out = m(torch.ones(6, 2, 4, requires_grad=True), torch.zeros(6, 2, dtype=torch.long))\n&gt;&gt;&gt; out\ntensor(1.3863, grad_fn=&lt;NllLossBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.MultiplicationFusion","title":"karbonn.modules.MultiplicationFusion","text":"<p>               Bases: <code>Module</code></p> <p>Implement a fusion layer that multiplies the inputs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import MultiplicationFusion\n&gt;&gt;&gt; module = MultiplicationFusion()\n&gt;&gt;&gt; module\nMultiplicationFusion()\n&gt;&gt;&gt; x1 = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n&gt;&gt;&gt; x2 = torch.tensor([[12.0, 13.0, 14.0], [15.0, 16.0, 17.0]], requires_grad=True)\n&gt;&gt;&gt; out = module(x1, x2)\n&gt;&gt;&gt; out\ntensor([[ 24.,  39.,  56.],\n        [ 75.,  96., 119.]], grad_fn=&lt;MulBackward0&gt;)\n&gt;&gt;&gt; out.mean().backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.NLinear","title":"karbonn.modules.NLinear","text":"<p>               Bases: <code>Module</code></p> <p>Implement N separate linear layers.</p> <p>Technically, <code>NLinear(n, in, out)</code> is just a layout of <code>n</code> linear layers <code>torch.nn.Linear(in, out)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of separate linear layers.</p> required <code>in_features</code> <code>int</code> <p>The size of each input sample.</p> required <code>out_features</code> <code>int</code> <p>The size of each output sample.</p> required <code>bias</code> <code>bool</code> <p>If set to <code>False</code>, the layer will not learn an additive bias.</p> <code>True</code> Shape <ul> <li>Input: <code>(*, n, in_features)</code>, where <code>*</code> means any number of     dimensions.</li> <li>Output: <code>(*, n, out_features)</code>,  where <code>*</code> has     the same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import NLinear\n&gt;&gt;&gt; # Example with 1 feature\n&gt;&gt;&gt; m = NLinear(n=3, in_features=4, out_features=6)\n&gt;&gt;&gt; m\nNLinear(n=3, in_features=4, out_features=6, bias=True)\n&gt;&gt;&gt; out = m(torch.randn(2, 3, 4))\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 3, 6])\n&gt;&gt;&gt; out = m(torch.randn(2, 5, 3, 4))\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 5, 3, 6])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.PiecewiseLinearNumericalEncoder","title":"karbonn.modules.PiecewiseLinearNumericalEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Implement a numerical encoder using piecewise linear functions.</p> <p>This layer was proposed in the following paper:</p> <p>On Embeddings for Numerical Features in Tabular Deep Learning Yury Gorishniy, Ivan Rubachev, Artem Babenko NeurIPS 2022, https://arxiv.org/pdf/2203.05556 https://github.com/yandex-research/rtdl-num-embeddings</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>Tensor</code> <p>The bins used to compute the piecewise linear representations. This input should be a tensor of shape <code>(n_features, n_bins)</code> or <code>(n_bins,)</code>. The bin values are sorted by ascending order for each feature.</p> required Shape <ul> <li>Input: <code>(*, n_features)</code>, where <code>*</code> means any number of     dimensions.</li> <li>Output: <code>(*, n_features, n_bins - 1)</code>,  where <code>*</code> has     the same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import PiecewiseLinearNumericalEncoder\n&gt;&gt;&gt; # Example with 1 feature\n&gt;&gt;&gt; m = PiecewiseLinearNumericalEncoder(bins=torch.tensor([[1.0, 2.0, 4.0, 8.0]]))\n&gt;&gt;&gt; m\nPiecewiseLinearNumericalEncoder(n_features=1, feature_size=3)\n&gt;&gt;&gt; out = m(torch.tensor([[0.0], [1.0], [2.0], [3.0]]))\n&gt;&gt;&gt; out\ntensor([[[-1.0000,  0.0000,  0.0000]],\n        [[ 0.0000,  0.0000,  0.0000]],\n        [[ 1.0000,  0.0000,  0.0000]],\n        [[ 1.0000,  0.5000,  0.0000]]])\n&gt;&gt;&gt; # Example with 2 features\n&gt;&gt;&gt; m = PiecewiseLinearNumericalEncoder(\n...     bins=torch.tensor([[1.0, 2.0, 4.0, 8.0], [0.0, 2.0, 4.0, 6.0]])\n... )\n&gt;&gt;&gt; m\nPiecewiseLinearNumericalEncoder(n_features=2, feature_size=3)\n&gt;&gt;&gt; out = m(torch.tensor([[0.0, 0.0], [1.0, 1.0], [2.0, 2.0], [3.0, 3.0]]))\n&gt;&gt;&gt; out\ntensor([[[-1.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000]],\n        [[ 0.0000,  0.0000,  0.0000],\n         [ 0.5000,  0.0000,  0.0000]],\n        [[ 1.0000,  0.0000,  0.0000],\n         [ 1.0000,  0.0000,  0.0000]],\n        [[ 1.0000,  0.5000,  0.0000],\n         [ 1.0000,  0.5000,  0.0000]]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.PiecewiseLinearNumericalEncoder.input_size","title":"karbonn.modules.PiecewiseLinearNumericalEncoder.input_size  <code>property</code>","text":"<pre><code>input_size: int\n</code></pre> <p>Return the input feature size i.e. the number of scalar values.</p>"},{"location":"refs/modules/#karbonn.modules.PiecewiseLinearNumericalEncoder.output_size","title":"karbonn.modules.PiecewiseLinearNumericalEncoder.output_size  <code>property</code>","text":"<pre><code>output_size: int\n</code></pre> <p>Return the output feature size i.e. the number of bins minus one.</p>"},{"location":"refs/modules/#karbonn.modules.Quadratic","title":"karbonn.modules.Quadratic","text":"<p>               Bases: <code>BaseAlphaActivation</code></p> <p>Implement the Quadratic activation layer.</p> <p>Formula: <code>1 / (1 + (alpha * x)^2)</code></p> <p>This activation layer was proposed in the following paper:</p> <pre><code>Beyond Periodicity: Towards a Unifying Framework for Activations in Coordinate-MLPs.\nRamasinghe S., Lucey S.\nECCV 2022. (http://arxiv.org/pdf/2111.15135)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>num_parameters</code> <code>int</code> <p>The number of learnable parameters. Although it takes an integer as input, there is only two values are legitimate: <code>1</code>, or the number of channels at input.</p> <code>1</code> <code>init</code> <code>float</code> <p>The initial value of the learnable parameter(s).</p> <code>1.0</code> <code>learnable</code> <code>bool</code> <p>If <code>True</code>, the parameters are learnt during the training, otherwise they are fixed.</p> <code>True</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Quadratic\n&gt;&gt;&gt; m = Quadratic()\n&gt;&gt;&gt; m\nQuadratic(num_parameters=1, learnable=True)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[1.0000, 0.5000, 0.2000, 0.1000],\n        [0.0588, 0.0385, 0.0270, 0.0200]], grad_fn=&lt;MulBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.QuantileRegressionLoss","title":"karbonn.modules.QuantileRegressionLoss","text":"<p>               Bases: <code>Module</code></p> <p>Implement a loss module that computes the quantile regression loss.</p> <p>Parameters:</p> Name Type Description Default <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>q</code> <code>float</code> <p>The quantile value. <code>q=0.5</code> is equivalent to the Mean Absolute Error (MAE).</p> <code>0.5</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import QuantileRegressionLoss\n&gt;&gt;&gt; criterion = QuantileRegressionLoss()\n&gt;&gt;&gt; criterion\nQuantileRegressionLoss(reduction=mean, q=0.5)\n&gt;&gt;&gt; loss = criterion(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ReLUn","title":"karbonn.modules.ReLUn","text":"<p>               Bases: <code>Module</code></p> <p>Implement the ReLU-n module.</p> <p>The ReLU-n equation is: <code>ReLUn(x, n)=min(max(0,x),n)</code></p> <p>Parameters:</p> Name Type Description Default <code>max</code> <code>float</code> <p>The maximum value a.k.a. <code>n</code> in the equation above.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ReLUn\n&gt;&gt;&gt; m = ReLUn(max=5)\n&gt;&gt;&gt; m\nReLUn(max=5.0)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[0., 1., 2., 3.],\n        [4., 5., 5., 5.]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.RelativeLoss","title":"karbonn.modules.RelativeLoss","text":"<p>               Bases: <code>Module</code></p> <p>Implement a \"generic\" relative loss that takes as input a criterion.</p> <p>Parameters:</p> Name Type Description Default <code>criterion</code> <code>Module | dict</code> <p>The criterion or its configuration. This criterion should not have reduction to be compatible with the shapes of the prediction and targets.</p> required <code>indicator</code> <code>BaseRelativeIndicator | dict | None</code> <p>The name of the indicator function to use or its implementation. If <code>None</code>, <code>ClassicalRelativeIndicator</code> is instantiated.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the indicator is zero.</p> <code>1e-08</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import RelativeLoss\n&gt;&gt;&gt; from karbonn.modules.loss import ClassicalRelativeIndicator\n&gt;&gt;&gt; criterion = RelativeLoss(\n...     criterion=torch.nn.MSELoss(reduction=\"none\"),\n...     indicator=ClassicalRelativeIndicator(),\n... )\n&gt;&gt;&gt; criterion\nRelativeLoss(\n  eps=1e-08, reduction=mean\n  (criterion): MSELoss()\n  (indicator): ClassicalRelativeIndicator()\n)\n&gt;&gt;&gt; prediction = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(3, 5)\n&gt;&gt;&gt; loss = criterion(prediction, target)\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.RelativeMSELoss","title":"karbonn.modules.RelativeMSELoss","text":"<p>               Bases: <code>RelativeLoss</code></p> <p>Implement the relative MSE loss.</p> <p>Parameters:</p> Name Type Description Default <code>indicator</code> <code>BaseRelativeIndicator | dict | None</code> <p>The name of the indicator function to use or its implementation. If <code>None</code>, <code>ClassicalRelativeIndicator</code> is instantiated.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the indicator is zero.</p> <code>1e-08</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import RelativeMSELoss\n&gt;&gt;&gt; from karbonn.modules.loss import ClassicalRelativeIndicator\n&gt;&gt;&gt; criterion = RelativeMSELoss(indicator=ClassicalRelativeIndicator())\n&gt;&gt;&gt; criterion\nRelativeMSELoss(\n  eps=1e-08, reduction=mean\n  (criterion): MSELoss()\n  (indicator): ClassicalRelativeIndicator()\n)\n&gt;&gt;&gt; prediction = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(3, 5)\n&gt;&gt;&gt; loss = criterion(prediction, target)\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.RelativeSmoothL1Loss","title":"karbonn.modules.RelativeSmoothL1Loss","text":"<p>               Bases: <code>RelativeLoss</code></p> <p>Implement the relative smooth L1 loss.</p> <p>Parameters:</p> Name Type Description Default <code>indicator</code> <code>BaseRelativeIndicator | dict | None</code> <p>The name of the indicator function to use or its implementation. If <code>None</code>, <code>ClassicalRelativeIndicator</code> is instantiated.</p> <code>None</code> <code>reduction</code> <code>str</code> <p>The reduction strategy. The valid values are <code>'mean'</code>, <code>'none'</code>,  and <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum will be divided by the number of elements in the input, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>eps</code> <code>float</code> <p>An arbitrary small strictly positive number to avoid undefined results when the indicator is zero.</p> <code>1e-08</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import RelativeSmoothL1Loss\n&gt;&gt;&gt; from karbonn.modules.loss import ClassicalRelativeIndicator\n&gt;&gt;&gt; criterion = RelativeSmoothL1Loss(indicator=ClassicalRelativeIndicator())\n&gt;&gt;&gt; criterion\nRelativeSmoothL1Loss(\n  eps=1e-08, reduction=mean\n  (criterion): SmoothL1Loss()\n  (indicator): ClassicalRelativeIndicator()\n)\n&gt;&gt;&gt; prediction = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randn(3, 5)\n&gt;&gt;&gt; loss = criterion(prediction, target)\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;MeanBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ResidualBlock","title":"karbonn.modules.ResidualBlock","text":"<p>               Bases: <code>Module</code></p> <p>Implementation of a residual block.</p> <p>Parameters:</p> Name Type Description Default <code>residual</code> <code>Module | dict</code> <p>The residual mapping module or its configuration (dictionary).</p> required <code>skip</code> <code>Module | dict | None</code> <p>The skip mapping module or its configuration (dictionary). If <code>None</code>, the <code>Identity</code> module is used.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.modules import ResidualBlock\n&gt;&gt;&gt; m = ResidualBlock(residual=nn.Sequential(nn.Linear(4, 6), nn.ReLU(), nn.Linear(6, 4)))\n&gt;&gt;&gt; m\nResidualBlock(\n  (residual): Sequential(\n    (0): Linear(in_features=4, out_features=6, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=6, out_features=4, bias=True)\n  )\n  (skip): Identity()\n)\n&gt;&gt;&gt; out = m(torch.rand(6, 4))\n&gt;&gt;&gt; out\ntensor([[...]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.SafeExp","title":"karbonn.modules.SafeExp","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the exponential of the elements.</p> <p>The values that are higher than the specified minimum value are set to this maximum value. Using a not too large positive value leads to an output tensor without Inf.</p> <p>Parameters:</p> Name Type Description Default <code>max</code> <code>float</code> <p>The maximum value before to compute the exponential.</p> <code>20.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import SafeExp\n&gt;&gt;&gt; m = SafeExp()\n&gt;&gt;&gt; m\nSafeExp(max=20.0)\n&gt;&gt;&gt; out = m(torch.tensor([[0.01, 0.1, 1.0], [10.0, 100.0, 1000.0]]))\n&gt;&gt;&gt; out\ntensor([[1.0101e+00, 1.1052e+00, 2.7183e+00],\n        [2.2026e+04, 4.8517e+08, 4.8517e+08]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.SafeLog","title":"karbonn.modules.SafeLog","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the logarithm natural of the elements.</p> <p>The values that are lower than the specified minimum value are set to this minimum value. Using a small positive value leads to an output tensor without NaN or Inf.</p> <p>Parameters:</p> Name Type Description Default <code>min</code> <code>float</code> <p>The minimum value before to compute the logarithm natural.</p> <code>1e-08</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import SafeLog\n&gt;&gt;&gt; m = SafeLog()\n&gt;&gt;&gt; m\nSafeLog(min=1e-08)\n&gt;&gt;&gt; out = m(torch.tensor([[1e-4, 1e-5, 1e-6], [1e-8, 1e-9, 1e-10]]))\n&gt;&gt;&gt; out\ntensor([[ -9.2103, -11.5129, -13.8155],\n        [-18.4207, -18.4207, -18.4207]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Sin","title":"karbonn.modules.Sin","text":"<p>               Bases: <code>Module</code></p> <p>Implement the sine activation layer.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Sin\n&gt;&gt;&gt; m = Sin()\n&gt;&gt;&gt; m\nSin()\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[ 0.0000,  0.8415,  0.9093,  0.1411],\n        [-0.7568, -0.9589, -0.2794,  0.6570]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Sinh","title":"karbonn.modules.Sinh","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute the hyperbolic sine (sinh) of the elements.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Sinh\n&gt;&gt;&gt; m = Sinh()\n&gt;&gt;&gt; m\nSinh()\n&gt;&gt;&gt; out = m(torch.tensor([[-1.0, 0.0, 1.0], [-2.0, 2.0, 4.0]]))\n&gt;&gt;&gt; out\ntensor([[-1.1752,  0.0000,  1.1752],\n        [-3.6269,  3.6269, 27.2899]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Snake","title":"karbonn.modules.Snake","text":"<p>               Bases: <code>Module</code></p> <p>Implement the Snake activation layer.</p> <p>Snake was proposed in the following paper:</p> <pre><code>Neural Networks Fail to Learn Periodic Functions and How to Fix It.\nZiyin L., Hartwig T., Ueda M.\nNeurIPS, 2020. (http://arxiv.org/pdf/2006.08195)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>frequency</code> <code>float</code> <p>The frequency.</p> <code>1.0</code> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Snake\n&gt;&gt;&gt; m = Snake()\n&gt;&gt;&gt; m\nSnake(frequency=1.0)\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[0.0000, 1.7081, 2.8268, 3.0199],\n        [4.5728, 5.9195, 6.0781, 7.4316]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.SquaredReLU","title":"karbonn.modules.SquaredReLU","text":"<p>               Bases: <code>Module</code></p> <p>Implement the Squared ReLU.</p> <p>Squared ReLU is defined in the following paper:</p> <pre><code>Primer: Searching for Efficient Transformers for Language Modeling.\nSo DR., Ma\u0144ke W., Liu H., Dai Z., Shazeer N., Le QV.\nNeurIPS, 2021. (https://arxiv.org/pdf/2109.08668.pdf)\n</code></pre> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import SquaredReLU\n&gt;&gt;&gt; m = SquaredReLU()\n&gt;&gt;&gt; m\nSquaredReLU()\n&gt;&gt;&gt; out = m(torch.arange(8, dtype=torch.float).view(2, 4))\n&gt;&gt;&gt; out\ntensor([[ 0.,  1.,  4.,  9.],\n        [16., 25., 36., 49.]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.Squeeze","title":"karbonn.modules.Squeeze","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to squeeze the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int | None</code> <p>The dimension to squeeze the input tensor. If <code>None</code>, all the dimensions of the input tensor of size 1 are removed.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import Squeeze\n&gt;&gt;&gt; m = Squeeze()\n&gt;&gt;&gt; m\nSqueeze(dim=None)\n&gt;&gt;&gt; out = m(torch.ones(2, 1, 3, 1))\n&gt;&gt;&gt; out.shape\ntorch.Size([2, 3])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.SumFusion","title":"karbonn.modules.SumFusion","text":"<p>               Bases: <code>Module</code></p> <p>Implement a layer to sum the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>normalized</code> <code>bool</code> <p>The output is normalized by the number of inputs.</p> <code>False</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import SumFusion\n&gt;&gt;&gt; module = SumFusion()\n&gt;&gt;&gt; module\nSumFusion(normalized=False)\n&gt;&gt;&gt; x1 = torch.tensor([[2.0, 3.0, 4.0], [5.0, 6.0, 7.0]], requires_grad=True)\n&gt;&gt;&gt; x2 = torch.tensor([[12.0, 13.0, 14.0], [15.0, 16.0, 17.0]], requires_grad=True)\n&gt;&gt;&gt; out = module(x1, x2)\n&gt;&gt;&gt; out\ntensor([[14., 16., 18.],\n        [20., 22., 24.]], grad_fn=&lt;AddBackward0&gt;)\n&gt;&gt;&gt; out.mean().backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ToBinaryLabel","title":"karbonn.modules.ToBinaryLabel","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute binary labels from scores by thresholding.</p> <p>The output label is <code>1</code> if the value is greater than the threshold, and <code>0</code> otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The threshold value used to compute the binary labels.</p> <code>0.0</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ToBinaryLabel\n&gt;&gt;&gt; transform = ToBinaryLabel()\n&gt;&gt;&gt; transform\nToBinaryLabel(threshold=0.0)\n&gt;&gt;&gt; out = transform(torch.tensor([-1.0, 1.0, -2.0, 1.0]))\n&gt;&gt;&gt; out\ntensor([0, 1, 0, 1])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ToBinaryLabel.threshold","title":"karbonn.modules.ToBinaryLabel.threshold  <code>property</code>","text":"<pre><code>threshold: float\n</code></pre> <p>The threshold used to compute the binary label.</p>"},{"location":"refs/modules/#karbonn.modules.ToBinaryLabel.forward","title":"karbonn.modules.ToBinaryLabel.forward","text":"<pre><code>forward(scores: Tensor) -&gt; Tensor\n</code></pre> <p>Compute binary labels from scores.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Tensor</code> <p>The scores used to compute the binary labels. This input must be a <code>torch.Tensor</code> of type float and shape <code>(d0, d1, ..., dn)</code></p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed binary labels where the values are <code>0</code> and <code>1</code>. The output is a <code>torch.Tensor</code> of type long and shape <code>(d0, d1, ..., dn)</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ToBinaryLabel\n&gt;&gt;&gt; transform = ToBinaryLabel()\n&gt;&gt;&gt; out = transform(torch.tensor([-1.0, 1.0, -2.0, 1.0]))\n&gt;&gt;&gt; out\ntensor([0, 1, 0, 1])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ToCategoricalLabel","title":"karbonn.modules.ToCategoricalLabel","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to compute categorical labels from scores.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ToCategoricalLabel\n&gt;&gt;&gt; transform = ToCategoricalLabel()\n&gt;&gt;&gt; transform\nToCategoricalLabel()\n&gt;&gt;&gt; out = transform(torch.tensor([[1.0, 2.0, 3.0, 4.0], [5.0, 3.0, 2.0, 2.0]]))\n&gt;&gt;&gt; out\ntensor([3, 0])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ToCategoricalLabel.forward","title":"karbonn.modules.ToCategoricalLabel.forward","text":"<pre><code>forward(scores: Tensor) -&gt; Tensor\n</code></pre> <p>Compute categorical labels from scores.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>Tensor</code> <p>The scores used to compute the categorical labels. This input must be a <code>torch.Tensor</code> of shape <code>(d0, d1, ..., dn, num_classes)</code> and type float.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed categorical labels where the values are in <code>{0, 1, ..., num_classes-1}</code>. The output is a <code>torch.Tensor</code> of type long and shape <code>(d0, d1, ..., dn)</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ToCategoricalLabel\n&gt;&gt;&gt; transform = ToCategoricalLabel()\n&gt;&gt;&gt; out = transform(torch.tensor([[1.0, 2.0, 3.0, 4.0], [5.0, 3.0, 2.0, 2.0]]))\n&gt;&gt;&gt; out\ntensor([3, 0])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ToFloat","title":"karbonn.modules.ToFloat","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to convert a tensor to a float tensor.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ToFloat\n&gt;&gt;&gt; m = ToFloat()\n&gt;&gt;&gt; m\nToFloat()\n&gt;&gt;&gt; out = m(torch.tensor([[2, -1, 0], [1, 2, 3]]))\n&gt;&gt;&gt; out\ntensor([[ 2., -1.,  0.],\n        [ 1.,  2.,  3.]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.ToLong","title":"karbonn.modules.ToLong","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to convert a tensor to a long tensor.</p> Shape <ul> <li>Input: <code>(*)</code>, where <code>*</code> means any number of dimensions.</li> <li>Output: <code>(*)</code>, same shape as the input.</li> </ul> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import ToLong\n&gt;&gt;&gt; m = ToLong()\n&gt;&gt;&gt; m\nToLong()\n&gt;&gt;&gt; out = m(torch.tensor([[2.0, -1.0, 0.0], [1.0, 2.0, 3.0]]))\n&gt;&gt;&gt; out\ntensor([[ 2, -1,  0],\n        [ 1,  2,  3]])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.TransformedLoss","title":"karbonn.modules.TransformedLoss","text":"<p>               Bases: <code>Module</code></p> <p>Implement a loss function where the predictions and targets are transformed before to be fed to the loss function.</p> <p>Parameters:</p> Name Type Description Default <code>criterion</code> <code>Module | dict</code> <p>The criterion or its configuration. The loss has two inputs: predictions and targets.</p> required <code>prediction</code> <code>Module | dict | None</code> <p>The transformation for the predictions or its configuration. If <code>None</code>, the identity transformation is used.</p> <code>None</code> <code>target</code> <code>Module | dict | None</code> <p>The transformation for the targets or its configuration. If <code>None</code>, the identity transformation is used.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import TransformedLoss, Asinh\n&gt;&gt;&gt; criterion = TransformedLoss(\n...     criterion=torch.nn.SmoothL1Loss(),\n...     prediction=Asinh(),\n...     target=Asinh(),\n... )\n&gt;&gt;&gt; loss = criterion(torch.randn(2, 4, requires_grad=True), torch.randn(2, 4))\n&gt;&gt;&gt; loss\ntensor(..., grad_fn=&lt;SmoothL1LossBackward0&gt;)\n&gt;&gt;&gt; loss.backward()\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.View","title":"karbonn.modules.View","text":"<p>               Bases: <code>Module</code></p> <p>Implement a <code>torch.nn.Module</code> to return a new tensor with the same data as the input tensor but of a different shape.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...] | list[int]</code> <p>The desired shape.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.modules import View\n&gt;&gt;&gt; m = View(shape=(-1, 2, 3))\n&gt;&gt;&gt; m\nView(shape=(-1, 2, 3))\n&gt;&gt;&gt; out = m(torch.ones(4, 5, 2, 3))\n&gt;&gt;&gt; out.shape\ntorch.Size([20, 2, 3])\n</code></pre>"},{"location":"refs/modules/#karbonn.modules.binary_focal_loss","title":"karbonn.modules.binary_focal_loss","text":"<pre><code>binary_focal_loss(\n    alpha: float = 0.5,\n    gamma: float = 2.0,\n    reduction: str = \"mean\",\n    logits: bool = False,\n) -&gt; BinaryFocalLoss\n</code></pre> <p>Return an instantiated binary focal loss with a binary cross entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The weighting factor, which must be in the range <code>[0, 1]</code>.</p> <code>0.5</code> <code>gamma</code> <code>float</code> <p>The focusing parameter, which must be positive (<code>&gt;=0</code>).</p> <code>2.0</code> <code>reduction</code> <code>str</code> <p>The reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> <code>'mean'</code> <code>logits</code> <code>bool</code> <p>If <code>True</code>, the <code>torch.nn.BCEWithLogitsLoss</code> is used, otherwise <code>torch.nn.BCELoss</code> is used.</p> <code>False</code> <p>Returns:</p> Type Description <code>BinaryFocalLoss</code> <p>The instantiated binary focal loss.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.modules import binary_focal_loss\n&gt;&gt;&gt; criterion = binary_focal_loss()\n&gt;&gt;&gt; criterion\nBinaryFocalLoss(\n  alpha=0.5, gamma=2.0, reduction=mean\n  (loss): BCELoss()\n)\n&gt;&gt;&gt; criterion = binary_focal_loss(logits=True)\n&gt;&gt;&gt; criterion\nBinaryFocalLoss(\n  alpha=0.5, gamma=2.0, reduction=mean\n  (loss): BCEWithLogitsLoss()\n)\n</code></pre>"},{"location":"refs/root/","title":"karbonn","text":""},{"location":"refs/root/#karbonn","title":"karbonn","text":"<p>Root package.</p>"},{"location":"refs/utils/","title":"karbonn.utils","text":""},{"location":"refs/utils/#module","title":"Module","text":""},{"location":"refs/utils/#karbonn.utils.freeze_module","title":"karbonn.utils.freeze_module","text":"<pre><code>freeze_module(module: Module) -&gt; None\n</code></pre> <p>Freeze the parameters of the given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to freeze.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import freeze_module\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; for name, param in module.named_parameters():\n...     print(name, param.requires_grad)\n...\nweight False\nbias False\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.get_module_device","title":"karbonn.utils.get_module_device","text":"<pre><code>get_module_device(module: Module) -&gt; device\n</code></pre> <p>Get the device used by this module.</p> <p>This function assumes the module uses a single device. If the module uses several devices, you should use <code>get_module_devices</code>. It returns <code>torch.device('cpu')</code> if the model does not have parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>device</code> <p>The device</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import get_module_device\n&gt;&gt;&gt; get_module_device(torch.nn.Linear(4, 6))\ndevice(type='cpu')\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.get_module_devices","title":"karbonn.utils.get_module_devices","text":"<pre><code>get_module_devices(module: Module) -&gt; tuple[device, ...]\n</code></pre> <p>Get the devices used in a module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>tuple[device, ...]</code> <p>The tuple of <code>torch.device</code>s used in the module.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import get_module_devices\n&gt;&gt;&gt; get_module_devices(torch.nn.Linear(4, 6))\n(device(type='cpu'),)\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.has_learnable_parameters","title":"karbonn.utils.has_learnable_parameters","text":"<pre><code>has_learnable_parameters(module: Module) -&gt; bool\n</code></pre> <p>Indicate if the module has learnable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module has at least one learnable parameter, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import has_learnable_parameters, freeze_module\n&gt;&gt;&gt; has_learnable_parameters(torch.nn.Linear(4, 6))\nTrue\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; has_learnable_parameters(module)\nFalse\n&gt;&gt;&gt; has_learnable_parameters(torch.nn.Identity())\nFalse\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.has_parameters","title":"karbonn.utils.has_parameters","text":"<pre><code>has_parameters(module: Module) -&gt; bool\n</code></pre> <p>Indicate if the module has parameters.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the module has at least one parameter, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import has_parameters\n&gt;&gt;&gt; has_parameters(torch.nn.Linear(4, 6))\nTrue\n&gt;&gt;&gt; has_parameters(torch.nn.Identity())\nFalse\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_module_on_device","title":"karbonn.utils.is_module_on_device","text":"<pre><code>is_module_on_device(module: Module, device: device) -&gt; bool\n</code></pre> <p>Indicate if all the parameters of a module are on the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <code>device</code> <code>device</code> <p>The device.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if all the parameters of the module are on the specified device, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import is_module_on_device\n&gt;&gt;&gt; is_module_on_device(torch.nn.Linear(4, 6), torch.device(\"cpu\"))\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.module_mode","title":"karbonn.utils.module_mode","text":"<pre><code>module_mode(module: Module) -&gt; Generator[None]\n</code></pre> <p>Implement a context manager that restores the mode (train or eval) of every submodule individually.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to restore the mode.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import module_mode\n&gt;&gt;&gt; module = torch.nn.ModuleDict(\n...     {\"module1\": torch.nn.Linear(4, 6), \"module2\": torch.nn.Linear(2, 4).eval()}\n... )\n&gt;&gt;&gt; print(module[\"module1\"].training, module[\"module2\"].training)\nTrue False\n&gt;&gt;&gt; with module_mode(module):\n...     module.eval()\n...     print(module[\"module1\"].training, module[\"module2\"].training)\n...\nModuleDict(\n  (module1): Linear(in_features=4, out_features=6, bias=True)\n  (module2): Linear(in_features=2, out_features=4, bias=True)\n)\nFalse False\n&gt;&gt;&gt; print(module[\"module1\"].training, module[\"module2\"].training)\nTrue False\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.num_learnable_parameters","title":"karbonn.utils.num_learnable_parameters","text":"<pre><code>num_learnable_parameters(module: Module) -&gt; int\n</code></pre> <p>Return the number of learnable parameters in the module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to compute the number of learnable parameters.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of learnable parameters.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import num_learnable_parameters\n&gt;&gt;&gt; num_learnable_parameters(torch.nn.Linear(4, 6))\n30\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; freeze_module(module)\n&gt;&gt;&gt; num_learnable_parameters(module)\n0\n&gt;&gt;&gt; num_learnable_parameters(torch.nn.Identity())\n0\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.num_parameters","title":"karbonn.utils.num_parameters","text":"<pre><code>num_parameters(module: Module) -&gt; int\n</code></pre> <p>Return the number of parameters in the module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to compute the number of parameters.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of parameters.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import num_parameters\n&gt;&gt;&gt; num_parameters(torch.nn.Linear(4, 6))\n30\n&gt;&gt;&gt; num_parameters(torch.nn.Identity())\n0\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.top_module_mode","title":"karbonn.utils.top_module_mode","text":"<pre><code>top_module_mode(module: Module) -&gt; Generator[None]\n</code></pre> <p>Implement a context manager that restores the mode (train or eval) of a given module.</p> <p>This context manager only restores the mode at the top-level.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to restore the mode.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import top_module_mode\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; print(module.training)\nTrue\n&gt;&gt;&gt; with top_module_mode(module):\n...     module.eval()\n...     print(module.training)\n...\nLinear(in_features=4, out_features=6, bias=True)\nFalse\n&gt;&gt;&gt; print(module.training)\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.unfreeze_module","title":"karbonn.utils.unfreeze_module","text":"<pre><code>unfreeze_module(module: Module) -&gt; None\n</code></pre> <p>Unfreeze the parameters of the given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to unfreeze.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import unfreeze_module\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; unfreeze_module(module)\n&gt;&gt;&gt; for name, param in module.named_parameters():\n...     print(name, param.requires_grad)\n...\nweight True\nbias True\n</code></pre>"},{"location":"refs/utils/#factory","title":"Factory","text":""},{"location":"refs/utils/#karbonn.utils.create_sequential","title":"karbonn.utils.create_sequential","text":"<pre><code>create_sequential(\n    modules: Sequence[Module | dict],\n) -&gt; Sequential\n</code></pre> <p>Create a <code>torch.nn.Sequential</code> from a sequence of modules.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>Sequence[Module | dict]</code> <p>The sequence of modules or their configuration.</p> required <p>Returns:</p> Type Description <code>Sequential</code> <p>The instantiated <code>torch.nn.Sequential</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.utils.factory import create_sequential\n&gt;&gt;&gt; seq = create_sequential(\n...     [\n...         {\"_target_\": \"torch.nn.Linear\", \"in_features\": 4, \"out_features\": 6},\n...         {\"_target_\": \"torch.nn.ReLU\"},\n...         {\"_target_\": \"torch.nn.Linear\", \"in_features\": 6, \"out_features\": 6},\n...     ]\n... )\n&gt;&gt;&gt; seq\nSequential(\n  (0): Linear(in_features=4, out_features=6, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=6, out_features=6, bias=True)\n)\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_module_config","title":"karbonn.utils.is_module_config","text":"<pre><code>is_module_config(config: dict) -&gt; bool\n</code></pre> <p>Indicate if the input configuration is a configuration for a <code>torch.nn.Module</code>.</p> <p>This function only checks if the value of the key  <code>_target_</code> is valid. It does not check the other values. If <code>_target_</code> indicates a function, the returned type hint is used to check the class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the input configuration is a configuration for a <code>torch.nn.Module</code> object, otherwise <code>False</code>..</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.utils import is_module_config\n&gt;&gt;&gt; is_module_config({\"_target_\": \"torch.nn.Identity\"})\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.setup_module","title":"karbonn.utils.setup_module","text":"<pre><code>setup_module(module: Module | dict) -&gt; Module\n</code></pre> <p>Set up a <code>torch.nn.Module</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module | dict</code> <p>The module or its configuration.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The instantiated <code>torch.nn.Module</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.utils import setup_module\n&gt;&gt;&gt; linear = setup_module(\n...     {\"_target_\": \"torch.nn.Linear\", \"in_features\": 4, \"out_features\": 6}\n... )\n&gt;&gt;&gt; linear\nLinear(in_features=4, out_features=6, bias=True)\n</code></pre>"},{"location":"refs/utils/#state-dict","title":"State dict","text":""},{"location":"refs/utils/#karbonn.utils.find_module_state_dict","title":"karbonn.utils.find_module_state_dict","text":"<pre><code>find_module_state_dict(\n    state_dict: dict | list | tuple | set, module_keys: set\n) -&gt; dict\n</code></pre> <p>Try to find automatically the part of the state dict related to a module.</p> <p>The user should specify the set of module's keys: <code>set(module.state_dict().keys())</code>. This function assumes that the set of keys only exists at one location in the state dict. If the set of keys exists at several locations in the state dict, only the first one is returned.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict | list | tuple | set</code> <p>The state dict. This function is called recursively on this input to find the queried state dict.</p> required <code>module_keys</code> <code>set</code> <p>The set of module keys.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The part of the state dict related to a module if it is found, otherwise an empty dict.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import find_module_state_dict\n&gt;&gt;&gt; state = {\n...     \"model\": {\n...         \"weight\": 42,\n...         \"network\": {\n...             \"weight\": torch.ones(5, 4),\n...             \"bias\": 2 * torch.ones(5),\n...         },\n...     }\n... }\n&gt;&gt;&gt; module = torch.nn.Linear(4, 5)\n&gt;&gt;&gt; state_dict = find_module_state_dict(state, module_keys=set(module.state_dict().keys()))\n&gt;&gt;&gt; state_dict\n{'weight': tensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]]), 'bias': tensor([2., 2., 2., 2., 2.])}\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.load_state_dict_to_module","title":"karbonn.utils.load_state_dict_to_module","text":"<pre><code>load_state_dict_to_module(\n    state_dict: dict, module: Module, strict: bool = True\n) -&gt; None\n</code></pre> <p>Load a state dict into a given module.</p> <p>This function will automatically try to find the module state dict in the given state dict.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>dict</code> <p>The state dict.</p> required <code>module</code> <code>Module</code> <p>The module. This function changes the weights of this module.</p> required <code>strict</code> <code>bool</code> <p>whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module's <code>torch.nn.Module.state_dict</code> function.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils import load_state_dict_to_module\n&gt;&gt;&gt; state = {\n...     \"model\": {\n...         \"weight\": 42,\n...         \"network\": {\n...             \"weight\": torch.ones(5, 4),\n...             \"bias\": 2 * torch.ones(5),\n...         },\n...     }\n... }\n&gt;&gt;&gt; module = torch.nn.Linear(4, 5)\n&gt;&gt;&gt; load_state_dict_to_module(state, module)\n&gt;&gt;&gt; out = module(torch.ones(2, 4))\n&gt;&gt;&gt; out\ntensor([[6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6.]], grad_fn=&lt;AddmmBackward0&gt;)\n</code></pre>"},{"location":"refs/utils/#loss","title":"Loss","text":""},{"location":"refs/utils/#karbonn.utils.is_loss_decreasing","title":"karbonn.utils.is_loss_decreasing","text":"<pre><code>is_loss_decreasing(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    optimizer: Optimizer,\n    feature: Tensor,\n    target: Tensor,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to update the weights of the model.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing\n&gt;&gt;&gt; module = nn.Linear(4, 2)\n&gt;&gt;&gt; is_loss_decreasing(\n...     module=module,\n...     criterion=nn.MSELoss(),\n...     optimizer=SGD(module.parameters(), lr=0.01),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_loss_decreasing_with_adam","title":"karbonn.utils.is_loss_decreasing_with_adam","text":"<pre><code>is_loss_decreasing_with_adam(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    feature: Tensor,\n    target: Tensor,\n    lr: float = 0.0003,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>The module is trained with the Adam optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.0003</code> <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing_with_adam\n&gt;&gt;&gt; is_loss_decreasing_with_adam(\n...     module=nn.Linear(4, 2),\n...     criterion=nn.MSELoss(),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n...     lr=0.0003,\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.is_loss_decreasing_with_sgd","title":"karbonn.utils.is_loss_decreasing_with_sgd","text":"<pre><code>is_loss_decreasing_with_sgd(\n    module: Module,\n    criterion: Module | Callable[[Tensor, Tensor], Tensor],\n    feature: Tensor,\n    target: Tensor,\n    lr: float = 0.01,\n    num_iterations: int = 1,\n    random_seed: int = 10772155803920552556,\n) -&gt; bool\n</code></pre> <p>Check if the loss decreased after some iterations.</p> <p>The module is trained with the <code>torch.optim.SGD</code> optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module to test. The module must have a single input tensor and a single output tensor.</p> required <code>criterion</code> <code>Module | Callable[[Tensor, Tensor], Tensor]</code> <p>The criterion to test.</p> required <code>feature</code> <code>Tensor</code> <p>The input of the module.</p> required <code>target</code> <code>Tensor</code> <p>The target used to compute the loss.</p> required <code>num_iterations</code> <code>int</code> <p>The number of optimization steps.</p> <code>1</code> <code>lr</code> <code>float</code> <p>The learning rate.</p> <code>0.01</code> <code>random_seed</code> <code>int</code> <p>The random seed to make the function deterministic if the module contains randomness.</p> <code>10772155803920552556</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the loss decreased after some iterations, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils import is_loss_decreasing_with_adam\n&gt;&gt;&gt; is_loss_decreasing_with_adam(\n...     module=nn.Linear(4, 2),\n...     criterion=nn.MSELoss(),\n...     feature=torch.rand(4, 4),\n...     target=torch.rand(4, 2),\n...     lr=0.01,\n... )\nTrue\n</code></pre>"},{"location":"refs/utils/#inputoutput-sizes","title":"Input/output sizes","text":""},{"location":"refs/utils/#karbonn.utils.size.find_in_features","title":"karbonn.utils.size.find_in_features","text":"<pre><code>find_in_features(module: Module) -&gt; list[int]\n</code></pre> <p>Find the input feature sizes of a given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The input feature sizes.</p> <p>Raises:</p> Type Description <code>SizeNotFound</code> <p>if the input feature sizes could not be found.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import find_in_features\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; in_features = find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; module = torch.nn.Bilinear(in1_features=4, in2_features=6, out_features=8)\n&gt;&gt;&gt; in_features = find_in_features(module)\n&gt;&gt;&gt; in_features\n[4, 6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.find_out_features","title":"karbonn.utils.size.find_out_features","text":"<pre><code>find_out_features(module: Module) -&gt; list[int]\n</code></pre> <p>Find the output feature sizes of a given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The output feature sizes.</p> <p>Raises:</p> Type Description <code>SizeNotFound</code> <p>if the output feature sizes could not be found.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import find_out_features\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; out_features = find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n&gt;&gt;&gt; module = torch.nn.Bilinear(in1_features=4, in2_features=6, out_features=8)\n&gt;&gt;&gt; out_features = find_out_features(module)\n&gt;&gt;&gt; out_features\n[8]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.BaseSizeFinder","title":"karbonn.utils.size.BaseSizeFinder","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> <p>Define the base class to find the input or output feature size of a module.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import AutoSizeFinder\n&gt;&gt;&gt; size_finder = AutoSizeFinder()\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.BaseSizeFinder.find_in_features","title":"karbonn.utils.size.BaseSizeFinder.find_in_features  <code>abstractmethod</code>","text":"<pre><code>find_in_features(module: T) -&gt; list[int]\n</code></pre> <p>Find the input feature sizes of a given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>T</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The input feature sizes.</p> <p>Raises:</p> Type Description <code>SizeNotFound</code> <p>if the input feature size could not be found.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import AutoSizeFinder\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; size_finder = AutoSizeFinder()\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; module = torch.nn.Bilinear(in1_features=4, in2_features=6, out_features=8)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4, 6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.BaseSizeFinder.find_out_features","title":"karbonn.utils.size.BaseSizeFinder.find_out_features  <code>abstractmethod</code>","text":"<pre><code>find_out_features(module: T) -&gt; list[int]\n</code></pre> <p>Find the output feature sizes of a given module.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>T</code> <p>The module.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The output feature sizes.</p> <p>Raises:</p> Type Description <code>SizeNotFoundError</code> <p>if the output feature size could not be found.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import AutoSizeFinder\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; size_finder = AutoSizeFinder()\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n&gt;&gt;&gt; module = torch.nn.Bilinear(in1_features=4, in2_features=6, out_features=8)\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[8]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.AutoSizeFinder","title":"karbonn.utils.size.AutoSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder</code></p> <p>Implement a size finder that automatically finds the size based on the module type.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import AutoSizeFinder\n&gt;&gt;&gt; size_finder = AutoSizeFinder()\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.AutoSizeFinder.add_size_finder","title":"karbonn.utils.size.AutoSizeFinder.add_size_finder  <code>classmethod</code>","text":"<pre><code>add_size_finder(\n    module_type: type[Module],\n    size_finder: BaseSizeFinder,\n    exist_ok: bool = False,\n) -&gt; None\n</code></pre> <p>Add a size finder for a given module type.</p> <p>Parameters:</p> Name Type Description Default <code>module_type</code> <code>type[Module]</code> <p>The module type.</p> required <code>size_finder</code> <code>BaseSizeFinder</code> <p>The size finder to use for the given module type.</p> required <code>exist_ok</code> <code>bool</code> <p>If <code>False</code>, <code>RuntimeError</code> is raised if the data type already exists. This parameter should be set to <code>True</code> to overwrite the size finder for a module type.</p> <code>False</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if a size finder is already registered for the module type and <code>exist_ok=False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils.size import AutoSizeFinder, LinearSizeFinder\n&gt;&gt;&gt; AutoSizeFinder.add_size_finder(nn.Linear, LinearSizeFinder(), exist_ok=True)\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.AutoSizeFinder.find_size_finder","title":"karbonn.utils.size.AutoSizeFinder.find_size_finder  <code>classmethod</code>","text":"<pre><code>find_size_finder(\n    module_type: type[Module],\n) -&gt; BaseSizeFinder\n</code></pre> <p>Find the size finder associated to a module type.</p> <p>Parameters:</p> Name Type Description Default <code>module_type</code> <code>type[Module]</code> <p>The module type.</p> required <p>Returns:</p> Type Description <code>BaseSizeFinder</code> <p>The size finder associated to the module type.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils.size import AutoSizeFinder\n&gt;&gt;&gt; AutoSizeFinder.find_size_finder(nn.Linear)\nLinearSizeFinder()\n&gt;&gt;&gt; AutoSizeFinder.find_size_finder(nn.Bilinear)\nBilinearSizeFinder()\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.AutoSizeFinder.has_size_finder","title":"karbonn.utils.size.AutoSizeFinder.has_size_finder  <code>classmethod</code>","text":"<pre><code>has_size_finder(module_type: type) -&gt; bool\n</code></pre> <p>Indicate if a size finder is registered for the given module type.</p> <p>Parameters:</p> Name Type Description Default <code>module_type</code> <code>type</code> <p>The module type.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if a size finder is registered, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; from karbonn.utils.size import AutoSizeFinder\n&gt;&gt;&gt; AutoSizeFinder.has_size_finder(nn.Linear)\nTrue\n&gt;&gt;&gt; AutoSizeFinder.has_size_finder(str)\nFalse\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.register_size_finders","title":"karbonn.utils.size.register_size_finders","text":"<pre><code>register_size_finders() -&gt; None\n</code></pre> <p>Register size finders to <code>AutoSizeFinder</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.utils.size import AutoSizeFinder, register_size_finders\n&gt;&gt;&gt; register_size_finders()\n&gt;&gt;&gt; size_finder = AutoSizeFinder()\n&gt;&gt;&gt; size_finder\nAutoSizeFinder(\n  ...\n)\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.BatchNormSizeFinder","title":"karbonn.utils.size.BatchNormSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for BatchNorm layers like <code>torch.nn.BatchNorm1d</code>, <code>torch.nn.BatchNorm2d</code>, <code>torch.nn.BatchNorm3d</code>, or <code>torch.nn.SyncBatchNorm</code>.</p> <p>This module size finder assumes the module has a single input and output, and the input size is given by the attribute <code>num_features</code> and the output size is given by the attribute <code>num_features</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import BatchNormSizeFinder\n&gt;&gt;&gt; size_finder = BatchNormSizeFinder()\n&gt;&gt;&gt; module = torch.nn.BatchNorm1d(num_features=6)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[6]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.BilinearSizeFinder","title":"karbonn.utils.size.BilinearSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder</code></p> <p>Implement a size finder for <code>torch.nn.Bilinear</code> layer or similar layers.</p> <p>This module size finder assumes the module has two inputs and one output. The input sizes are given by the attribute <code>in1_features</code> and <code>in2_features</code> and the output size is given by the attribute <code>out_features</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import BilinearSizeFinder\n&gt;&gt;&gt; size_finder = BilinearSizeFinder()\n&gt;&gt;&gt; module = torch.nn.Bilinear(in1_features=4, in2_features=2, out_features=6)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4, 2]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.ConvolutionSizeFinder","title":"karbonn.utils.size.ConvolutionSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for convolution layers like <code>torch.nn.ConvNd</code> and <code>torch.nn.ConvTransposeNd</code>.</p> <p>This module size finder assumes the module has a single input and output, and the input size is given by the attribute <code>in_channels</code> and the output size is given by the attribute <code>out_channels</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import ConvolutionSizeFinder\n&gt;&gt;&gt; size_finder = ConvolutionSizeFinder()\n&gt;&gt;&gt; module = torch.nn.Conv2d(in_channels=4, out_channels=6, kernel_size=1)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.EmbeddingSizeFinder","title":"karbonn.utils.size.EmbeddingSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for embedding layers like <code>torch.nn.Embedding</code>.</p> <p>This module size finder assumes the module has a single input and output, and the input size is always 1, and the output size is given by the attribute <code>embedding_dim</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import EmbeddingSizeFinder\n&gt;&gt;&gt; size_finder = EmbeddingSizeFinder()\n&gt;&gt;&gt; module = torch.nn.Embedding(num_embeddings=5, embedding_dim=6)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[1]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.GroupNormSizeFinder","title":"karbonn.utils.size.GroupNormSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for Group Normalization layers like <code>torch.nn.GroupNorm</code>.</p> <p>This module size finder assumes the module has a single input and output, and the input size is given by the attribute <code>num_channels</code> and the output size is given by the attribute <code>num_channels</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import GroupNormSizeFinder\n&gt;&gt;&gt; size_finder = GroupNormSizeFinder()\n&gt;&gt;&gt; module = torch.nn.GroupNorm(num_groups=2, num_channels=8)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[8]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[8]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.LinearSizeFinder","title":"karbonn.utils.size.LinearSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for <code>torch.nn.Linear</code> layer or similar layers.</p> <p>This module size finder assumes the module has a single input and output, and the input size is given by the attribute <code>in_features</code> and the output size is given by the attribute <code>out_features</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import LinearSizeFinder\n&gt;&gt;&gt; size_finder = LinearSizeFinder()\n&gt;&gt;&gt; module = torch.nn.Linear(4, 6)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.ModuleListSizeFinder","title":"karbonn.utils.size.ModuleListSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[ModuleList]</code></p> <p>Implement a size finder for <code>torch.nn.ModuleList</code> layer or similar layers.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import ModuleListSizeFinder\n&gt;&gt;&gt; size_finder = ModuleListSizeFinder()\n&gt;&gt;&gt; module = nn.ModuleList(\n...     [nn.Linear(4, 6), nn.ReLU(), nn.LSTM(input_size=4, hidden_size=6)]\n... )\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.MultiheadAttentionSizeFinder","title":"karbonn.utils.size.MultiheadAttentionSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for <code>torch.nn.MultiheadAttention</code> layer or similar layers.</p> <p>This module size finder assumes the module has a single input and output, and the input size is given by the attribute <code>embed_dim</code> and the output size is given by the attribute <code>embed_dim</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import MultiheadAttentionSizeFinder\n&gt;&gt;&gt; size_finder = MultiheadAttentionSizeFinder()\n&gt;&gt;&gt; module = torch.nn.MultiheadAttention(embed_dim=4, num_heads=2)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4, 4, 4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[4]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.RecurrentSizeFinder","title":"karbonn.utils.size.RecurrentSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for recurrent layers like <code>torch.nn.RNN</code>, <code>torch.nn.GRU</code>, and <code>torch.nn.LSTM</code>.</p> <p>This module size finder assumes the module has a single input and output, and the input size is given by the attribute <code>input_size</code> and the output size is given by the attribute <code>hidden_size</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import RecurrentSizeFinder\n&gt;&gt;&gt; size_finder = RecurrentSizeFinder()\n&gt;&gt;&gt; module = torch.nn.RNN(input_size=4, hidden_size=6)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[6]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.SequentialSizeFinder","title":"karbonn.utils.size.SequentialSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Sequential]</code></p> <p>Implement a size finder for <code>torch.nn.Sequential</code> layer.</p> <p>This module size finder iterates over the child modules until to find one where it can compute the size.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import SequentialSizeFinder\n&gt;&gt;&gt; size_finder = SequentialSizeFinder()\n&gt;&gt;&gt; module = torch.nn.Sequential(\n...     torch.nn.Linear(4, 6), torch.nn.ReLU(), torch.nn.Linear(6, 8)\n... )\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[8]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.TransformerLayerSizeFinder","title":"karbonn.utils.size.TransformerLayerSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for layers like <code>torch.nn.TransformerEncoderLayer</code> or <code>torch.nn.TransformerDecoderLayer</code>.</p> <p>This module size finder assumes the module has an attribute <code>self_attn</code> which is used to find the input and output feature sizes.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import TransformerLayerSizeFinder\n&gt;&gt;&gt; size_finder = TransformerLayerSizeFinder()\n&gt;&gt;&gt; module = torch.nn.TransformerEncoderLayer(d_model=4, nhead=1)\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[4]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.TransformerSizeFinder","title":"karbonn.utils.size.TransformerSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder[Module]</code></p> <p>Implement a size finder for layers like <code>torch.nn.TransformerEncoder</code> or <code>torch.nn.TransformerDecoder</code>.</p> <p>This module size finder assumes the module has an attribute <code>self_attn</code> which is used to find the input and output feature sizes.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import TransformerSizeFinder\n&gt;&gt;&gt; size_finder = TransformerSizeFinder()\n&gt;&gt;&gt; module = torch.nn.TransformerEncoder(\n...     torch.nn.TransformerEncoderLayer(d_model=4, nhead=1),\n...     num_layers=1,\n... )\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)\n&gt;&gt;&gt; in_features\n[4]\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)\n&gt;&gt;&gt; out_features\n[4]\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.UnknownSizeFinder","title":"karbonn.utils.size.UnknownSizeFinder","text":"<p>               Bases: <code>BaseSizeFinder</code></p> <p>Implement a size finder for the modules where the input and output feature sizes are unknown.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from karbonn.utils.size import UnknownSizeFinder\n&gt;&gt;&gt; size_finder = UnknownSizeFinder()\n&gt;&gt;&gt; module = torch.nn.ReLU()\n&gt;&gt;&gt; in_features = size_finder.find_in_features(module)  # doctest: +SKIP\n&gt;&gt;&gt; out_features = size_finder.find_out_features(module)  # doctest: +SKIP\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.get_size_finders","title":"karbonn.utils.size.get_size_finders","text":"<pre><code>get_size_finders() -&gt; dict[type[Module], BaseSizeFinder]\n</code></pre> <p>Return the default mappings between the module types and their size finders.</p> <p>Returns:</p> Type Description <code>dict[type[Module], BaseSizeFinder]</code> <p>The default mappings between the module types and their size finders.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.utils.size import get_size_finders\n&gt;&gt;&gt; get_size_finders()\n{&lt;class 'torch.nn.modules.module.Module'&gt;: UnknownSizeFinder(), ...}\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.get_torch_size_finders","title":"karbonn.utils.size.get_torch_size_finders","text":"<pre><code>get_torch_size_finders() -&gt; (\n    dict[type[Module], BaseSizeFinder]\n)\n</code></pre> <p>Return the default mappings between the module types and their size finders.</p> <p>Returns:</p> Type Description <code>dict[type[Module], BaseSizeFinder]</code> <p>The default mappings between the module types and their size finders.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.utils.size import get_torch_size_finders\n&gt;&gt;&gt; get_torch_size_finders()\n{&lt;class 'torch.nn.modules.module.Module'&gt;: UnknownSizeFinder(), ...}\n</code></pre>"},{"location":"refs/utils/#karbonn.utils.size.get_karbonn_size_finders","title":"karbonn.utils.size.get_karbonn_size_finders","text":"<pre><code>get_karbonn_size_finders() -&gt; (\n    dict[type[Module], BaseSizeFinder]\n)\n</code></pre> <p>Return the default mappings between the module types and their size finders.</p> <p>Returns:</p> Type Description <code>dict[type[Module], BaseSizeFinder]</code> <p>The default mappings between the module types and their size finders.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from karbonn.utils.size import get_karbonn_size_finders\n&gt;&gt;&gt; get_karbonn_size_finders()\n{...}\n</code></pre>"}]}